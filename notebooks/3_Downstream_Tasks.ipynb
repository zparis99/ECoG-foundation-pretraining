{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01cc3608-a562-45df-a949-d80b1ee404a9",
   "metadata": {},
   "source": [
    "## Downstream Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289a2cb-13b5-4e69-86e8-bc285347ae19",
   "metadata": {},
   "source": [
    "This section is for our downstream tasks which we want our embeddings to do well on. Currently we have language encoding and decoding implemented. This works as follows:\n",
    "\n",
    "1. Generate a dataset with (GPT-2 embeddings of heard words, low dimensional embeddings of the neural data starting from some offset of the word onset).\n",
    "2. Train a linear model to either go from the word embeddings to neural data (encoding) or neural data to word embeddings (decoding).\n",
    "3. Report performance.\n",
    "\n",
    "Pretty simple! While language encoding and decoding are a big goal, we would like to try using the embeddings in simpler tasks first. Here are some ideas that you could try implementing and testing:\n",
    "\n",
    "1. A binary classifier where the model predicts if the subject is hearing a word at that moment or not. This would entail modifying the data loader to load neural signals which occur during word onsets and offsets and some that do not. Deciding how to balance the weights of those examples is up to you.\n",
    "2. A volume regressor which goes from the neural embedding to the volume of the sound that the subject is hearing at that moment. You would need to modify the data loader to grab neural signals and the average volume during that signal.\n",
    "3. Right now we only train a single linear layer on top of the model without passing gradients back to the model. It would be interesting to see how it performs if we finetune our model on these downstream tasks and could be implemented without much trouble.\n",
    "4. Anything else! You can see what kind of data we have access to in the data section below.\n",
    "\n",
    "P.S. Make sure the model is in eval mode (call model.eval()) before doing anything with it here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7de023-9534-41a7-9051-9bb8d6744b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local path to the github repo. Must be accessible from this notebook.\n",
    "# For compute cluster or local machine.\n",
    "path_to_github_repo = '../'\n",
    "\n",
    "# For Colab.\n",
    "# path_to_github_repo = 'ECoG-foundation-model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3607d48-e551-4e22-947a-bd67b668dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.join(path_to_github_repo, 'ECoG_MAE'))\n",
    "from mae_st_util.models_mae import MaskedAutoencoderViT\n",
    "from config import create_video_mae_experiment_config_from_file\n",
    "from ecog_setup import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffab326-3f82-4eb4-b271-2d3999281f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to train on.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501ee295-71f1-4b23-b43e-15b22b2940c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size (8, 8) patch_size (2, 2) frames 128 t_patch_size 4\n",
      "model initialized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedAutoencoderViT(\n",
       "  (masked_input_norm): MaskedBatchNorm(\n",
       "    (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  )\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv3d(1, 16, kernel_size=(4, 2, 2), stride=(4, 2, 2))\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x Block(\n",
       "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_embed): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-2): 3 x Block(\n",
       "      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder_pred): Linear(in_features=16, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the model and set it in eval mode.\n",
    "model_dir = \"models\"\n",
    "ecog_config = create_video_mae_experiment_config_from_file(os.path.join(model_dir, \"experiment_config.ini\"))\n",
    "\n",
    "model = create_model(ecog_config)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"model.pth\"), weights_only=True))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517a104-e33c-40c4-b9c3-ae7cfca2c48c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de875888-7464-4251-8484-e0c66ffc5f94",
   "metadata": {},
   "source": [
    "For this part of the project we have access to the data in word-embeddings/ and preprocessed-highgamma/. Note that the .mat files in preprocessed-highgamma already have been preprocessed to high-gamma so there's no need for more filtering here.\n",
    "\n",
    "You can load data in the preprocessed-highgamma data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e7eb619-01fd-423d-baa4-817bc82d398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(915602,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data for the first electrode\n",
    "data = scipy.io.loadmat('../preprocessed-highgamma/NY798_111_Part1_conversation1_electrode_preprocess_file_1.mat')['p1st'].flatten()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05363dee-bf26-4e24-8b0b-28e2e33bccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABC3ElEQVR4nO3deXwU5eHH8W8OcnAk4ZAE5IqKIpcKCEbQWkxFpFaFehVbRX9exQNotdIK3kJtvbABPBCsFRFaRVG5DDdyRsJNuEkgJFy5IOTc5/cHZskmm2Q32cxmk8/79drXC2ZmZ57M7O5855nnecbPGGMEAABgEX9vFwAAADQshA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUCvV2Asmw2m1JTU9WsWTP5+fl5uzgAAMAFxhjl5OSobdu28vevvG6jzoWP1NRUtW/f3tvFAAAA1ZCSkqJ27dpVukydCx/NmjWTdK7wYWFhXi4NAABwRXZ2ttq3b28/j1emzoWPklstYWFhhA8AAHyMK00maHAKAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALCU2+HjyJEjuu+++9SyZUuFhoaqR48e2rhxo32+MUbjx49XmzZtFBoaqtjYWO3Zs8ejhQYAAL7LrfCRkZGh/v37q1GjRpo/f7527NihN998U82bN7cv88Ybb2jSpEmaOnWq1q1bpyZNmmjQoEHKy8vzeOEBAIDv8TPGGFcXfu6557R69WqtXLnS6XxjjNq2bas//elP+vOf/yxJysrKUmRkpGbMmKF77rmnym1kZ2crPDxcWVlZjHAKAICPcOf87VbNxzfffKM+ffrozjvvVOvWrXXVVVfpww8/tM8/cOCA0tLSFBsba58WHh6ufv36ac2aNU7XmZ+fr+zsbIcXAACov9wKH/v379eUKVPUuXNnLVy4UI8//rieeuopffLJJ5KktLQ0SVJkZKTD+yIjI+3zypowYYLCw8PtL55oCwBA/eZW+LDZbOrVq5def/11XXXVVXrkkUf08MMPa+rUqdUuwNixY5WVlWV/paSkVHtdAADXzdmYolV7Tni7GGiA3Aofbdq0UdeuXR2mXX755UpOTpYkRUVFSZLS09MdlklPT7fPKys4ONj+BFueZAsA1th5NFvP/HeL7pu2zttFQQPkVvjo37+/kpKSHKbt3r1bHTt2lCRFR0crKipK8fHx9vnZ2dlat26dYmJiPFBcAIAnpGae9XYR0IAFurPw6NGjde211+r111/XXXfdpfXr1+uDDz7QBx98IEny8/PTqFGj9Oqrr6pz586Kjo7WuHHj1LZtW91+++21UX4AAOBj3AofV199tb766iuNHTtWL7/8sqKjo/XOO+9o+PDh9mWeffZZnTlzRo888ogyMzM1YMAALViwQCEhIR4vPAAA8D1ujfNhBcb5AIDaF78zXQ99cm506oMTh3i5NKgPam2cDwAAgJoifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AEADVLfGtkZDQ/gAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifAAAAEsRPgAAgKUIHwAAwFKEDwAAYCnCBwAAsBThAwAaIOPtAqBBI3wAAABLET4AAIClCB8AAMBShA8AaICModUHvIfwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAgAaI8U3hTYQPAABgKcIHAACwFOEDAABYivABAAAs5Vb4ePHFF+Xn5+fw6tKli31+Xl6eRo4cqZYtW6pp06YaNmyY0tPTPV5oAADgu9yu+ejWrZuOHj1qf61atco+b/To0Zo3b57mzJmj5cuXKzU1VUOHDvVogQEAgG8LdPsNgYGKiooqNz0rK0vTpk3TzJkzNXDgQEnS9OnTdfnll2vt2rW65ppral5aAADg89yu+dizZ4/atm2riy66SMOHD1dycrIkKSEhQYWFhYqNjbUv26VLF3Xo0EFr1qzxXIkBAIBPc6vmo1+/fpoxY4Yuu+wyHT16VC+99JKuu+46bdu2TWlpaQoKClJERITDeyIjI5WWllbhOvPz85Wfn2//f3Z2tnt/AQAA8CluhY/Bgwfb/92zZ0/169dPHTt21OzZsxUaGlqtAkyYMEEvvfRStd4LAAB8T4262kZEROjSSy/V3r17FRUVpYKCAmVmZjosk56e7rSNSImxY8cqKyvL/kpJSalJkQAAQB1Xo/Bx+vRp7du3T23atFHv3r3VqFEjxcfH2+cnJSUpOTlZMTExFa4jODhYYWFhDi8AAFB/uXXb5c9//rNuvfVWdezYUampqXrhhRcUEBCge++9V+Hh4XrooYc0ZswYtWjRQmFhYXryyScVExNDTxcAAGDnVvg4fPiw7r33Xp08eVIXXHCBBgwYoLVr1+qCCy6QJL399tvy9/fXsGHDlJ+fr0GDBmny5Mm1UnAAAOCb3Aofs2bNqnR+SEiI4uLiFBcXV6NCAQBqlzHeLgEaMp7tAgAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAA0So4zBewgfAADAUoQPAABgKcIHAACwFOEDAABYivABAA0QT7WFNxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+ACABsh4uwBo0AgfAADAUoQPAABgKcIHAACwFOEDAABYivABAD4mv6hYn649pOSTud4uClAtgd4uAADAPXFL92lS/B5J0sGJQ7xcGsB91HwAgI9Zu/+kt4sA1AjhAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfANAAGZ4sBy8ifABAA2R4ri28iPABAAAsVaPwMXHiRPn5+WnUqFH2aXl5eRo5cqRatmyppk2batiwYUpPT69pOQEAQD1R7fCxYcMGvf/+++rZs6fD9NGjR2vevHmaM2eOli9frtTUVA0dOrTGBQUAAPVDtcLH6dOnNXz4cH344Ydq3ry5fXpWVpamTZumt956SwMHDlTv3r01ffp0/fjjj1q7dq3HCg0AAHxXtcLHyJEjNWTIEMXGxjpMT0hIUGFhocP0Ll26qEOHDlqzZo3TdeXn5ys7O9vhBQAA6i+3n2o7a9Ys/fTTT9qwYUO5eWlpaQoKClJERITD9MjISKWlpTld34QJE/TSSy+5WwwAAOCj3Kr5SElJ0dNPP63PPvtMISEhHinA2LFjlZWVZX+lpKR4ZL0AAKBucit8JCQk6NixY+rVq5cCAwMVGBio5cuXa9KkSQoMDFRkZKQKCgqUmZnp8L709HRFRUU5XWdwcLDCwsIcXgCAivl5uwBADbl12+XGG2/U1q1bHaaNGDFCXbp00V/+8he1b99ejRo1Unx8vIYNGyZJSkpKUnJysmJiYjxXagAA4LPcCh/NmjVT9+7dHaY1adJELVu2tE9/6KGHNGbMGLVo0UJhYWF68sknFRMTo2uuucZzpQYAAD7L7QanVXn77bfl7++vYcOGKT8/X4MGDdLkyZM9vRkAAOCjahw+li1b5vD/kJAQxcXFKS4urqarBgAA9RDPdgEAAJYifABAA2R4qC28iPABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAfIwfj7WFjyN8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAA2Q8XYB0KARPgAAgKUIHwAAwFKEDwBogIzhxgu8h/ABAAAsRfgAAACWInwAAABLET4AAIClCB8A4GP85OftIgA1QvgAgAYur7DY20VAA0P4AIAGrs+rP3i7CGhgCB8A0MCdzi/ydhHQwBA+AACApQgfgAcUFdu8XQQA8BmED6CG5m46os7Pz9eCbWneLgoA+ATCB1BDo75IlDHSY/9J8HZRAMAnED4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAAJYifACAj/Hz83YJgJohfAAAAEsRPgAAgKUIHwAAwFKEDwAAYCm3wseUKVPUs2dPhYWFKSwsTDExMZo/f759fl5enkaOHKmWLVuqadOmGjZsmNLT0z1eaAAA4LvcCh/t2rXTxIkTlZCQoI0bN2rgwIG67bbbtH37dknS6NGjNW/ePM2ZM0fLly9Xamqqhg4dWisFBwBUnzHeLgEaskB3Fr711lsd/v/aa69pypQpWrt2rdq1a6dp06Zp5syZGjhwoCRp+vTpuvzyy7V27Vpdc801nis1AADwWdVu81FcXKxZs2bpzJkziomJUUJCggoLCxUbG2tfpkuXLurQoYPWrFlT4Xry8/OVnZ3t8AIAAPWX2+Fj69atatq0qYKDg/XYY4/pq6++UteuXZWWlqagoCBFREQ4LB8ZGam0tLQK1zdhwgSFh4fbX+3bt3f7jwAAAL7D7fBx2WWXKTExUevWrdPjjz+u+++/Xzt27Kh2AcaOHausrCz7KyUlpdrrAgAAdZ9bbT4kKSgoSJdccokkqXfv3tqwYYPeffdd3X333SooKFBmZqZD7Ud6erqioqIqXF9wcLCCg4PdLzkAAPBJNR7nw2azKT8/X71791ajRo0UHx9vn5eUlKTk5GTFxMTUdDMAAKCecKvmY+zYsRo8eLA6dOignJwczZw5U8uWLdPChQsVHh6uhx56SGPGjFGLFi0UFhamJ598UjExMfR0AYA6xoi+tvAet8LHsWPH9Ic//EFHjx5VeHi4evbsqYULF+pXv/qVJOntt9+Wv7+/hg0bpvz8fA0aNEiTJ0+ulYIDQEPFU23h69wKH9OmTat0fkhIiOLi4hQXF1ejQgEAgPqLZ7sAAABLET4AAIClCB8AAMBShA8AAGApwgcANEA81RbeRPgAAACWInwAAABLET4AAIClCB8AAMBShA8AAGApwgcAALAU4QMAAFiK8AEAACxF+AAAH+MnP28XAagRwgcAALAU4QMAfIwRY6PDtxE+AACApQgfAADAUoQPAABgKcIHADRAhmYj8CLCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAPgYP/l5uwhAjRA+AACApQgfAADAUoQPAGiAeLSL9fKLirX/+GlvF6NOIHwAAGCBu95fq4FvLteSXeneLorXET4AoAEyPNbWcptTMiVJczYe9m5B6gDCBwAAsBThAwAAWIrwAQAALEX4AAAAliJ8AABgIT8GqCV8AABgJYbHJ3wAAACLET4AAIClCB8A4GNoM+DjOH6EDwAAYC23wseECRN09dVXq1mzZmrdurVuv/12JSUlOSyTl5enkSNHqmXLlmratKmGDRum9HTGsQcAAOe4FT6WL1+ukSNHau3atVq8eLEKCwt100036cyZM/ZlRo8erXnz5mnOnDlavny5UlNTNXToUI8XHAAAX8RdFynQnYUXLFjg8P8ZM2aodevWSkhI0PXXX6+srCxNmzZNM2fO1MCBAyVJ06dP1+WXX661a9fqmmuu8VzJAQCAT6pRm4+srCxJUosWLSRJCQkJKiwsVGxsrH2ZLl26qEOHDlqzZo3TdeTn5ys7O9vhBQCoXTzT1nv8aDFc/fBhs9k0atQo9e/fX927d5ckpaWlKSgoSBEREQ7LRkZGKi0tzel6JkyYoPDwcPurffv21S0SAADwAdUOHyNHjtS2bds0a9asGhVg7NixysrKsr9SUlJqtD4AAFC3udXmo8QTTzyhb7/9VitWrFC7du3s06OiolRQUKDMzEyH2o/09HRFRUU5XVdwcLCCg4OrUwwAAHwON13crPkwxuiJJ57QV199pSVLlig6Otphfu/evdWoUSPFx8fbpyUlJSk5OVkxMTGeKTEAAD6MJh9u1nyMHDlSM2fO1Ndff61mzZrZ23GEh4crNDRU4eHheuihhzRmzBi1aNFCYWFhevLJJxUTE0NPFwAAIMnN8DFlyhRJ0g033OAwffr06XrggQckSW+//bb8/f01bNgw5efna9CgQZo8ebJHCgsAAHyfW+HDmKo7Z4WEhCguLk5xcXHVLhQAAKi/eLYLAACwFOEDAAAL0d6U8AEAgKUY4ZTwAQAALEb4AAC41KEA8BTCBwAAFuKmC+EDAABYjPABAA0Rd1ngRYQPAACsxH0XwgcANESGqg94EeEDQL1xOCNXi3ek03MDdZofVR+EDwD1x4C/L9XD/96oxTvSvV0UAJUgfACod9YfOOXtIgCoBOEDACDuVMFKhA8AACzEo10IHwDqIS7iUZeRPQgfAADAYoQPAPAxPJIdvo7wAQAALEX4AADAQlRcET4A1EN0GwXqNsIHADRABDTvYXh1wgcAQHRPhrUIHwAAwFKEDwD1Do+LR11Gg1PCBwD4tMzcAm8XAXAb4QMAfNjGgxneLgLgNsIHAAAW4rYL4QNAPUQ30vrLGFMPbjWRPggfAACf8einCbry5cVKTMn0dlFQA4QPAICMj1QXLdqRLkmavvqAl0uCmiB8AICPodIevo7wAQA+zDfqK1AaDU4JHwAAWIrsQfgAAAAWI3wAAABLET4AwIdVtwrfnbYix7LzdCw7r5pbAsoL9HYBAMDTfKXbqC8oLLap7+vxkqSkV29WcGCAl0vk+2hwSs0HAPi02o5ZOXlFTv8N1AThAwBQYYihFsnz/OjvQvgAAF+z/8Rpr2yXUyY8hfABoN6p79fqKafOWrat+r4v4R2EDwBogKpzN8WPlpIewW6sRvhYsWKFbr31VrVt21Z+fn6aO3euw3xjjMaPH682bdooNDRUsbGx2rNnj6fKCwCwUF1t8lFXywXXuB0+zpw5oyuuuEJxcXFO57/xxhuaNGmSpk6dqnXr1qlJkyYaNGiQ8vLoIw6g9mw4eMrbRaiXTKkbL1yww1PcHudj8ODBGjx4sNN5xhi98847ev7553XbbbdJkv79738rMjJSc+fO1T333FOz0gJABVbuOWH/N1fFHlRqX3K7wDPYjR5u83HgwAGlpaUpNjbWPi08PFz9+vXTmjVrnL4nPz9f2dnZDi8AQO36OvGI2++hi6hn0HbGw+EjLS1NkhQZGekwPTIy0j6vrAkTJig8PNz+at++vSeLBABwYt0B125TUYmE2uD13i5jx45VVlaW/ZWSkuLtIgFAg8OtKljJo+EjKipKkpSenu4wPT093T6vrODgYIWFhTm8AMBdVGRbgJ0MD/Fo+IiOjlZUVJTi4+Pt07Kzs7Vu3TrFxMR4clMAUCHDzQKPoUYEtcHt3i6nT5/W3r177f8/cOCAEhMT1aJFC3Xo0EGjRo3Sq6++qs6dOys6Olrjxo1T27Ztdfvtt3uy3ADQYHWJaqZdaTmSav/ZKw5dbetQzUddKou7fLnsnuJ2+Ni4caN++ctf2v8/ZswYSdL999+vGTNm6Nlnn9WZM2f0yCOPKDMzUwMGDNCCBQsUEhLiuVIDQAPWokmQZdsqnW04Z8JT3A4fN9xwQ6VJ28/PTy+//LJefvnlGhUMAFC31KUuonWnJKgOr/d2AQBPKH1etLqdgs1mlJ1XaO1GLVJ6V2acKfBaOVC/ED4AoIZ+//E69XxxkfYdt/5R91bWRhw4ccaybdVnDNZG+ACAGlu996QkafbG+jdOUW03aK2uunQLCO4jfAAAXOqeXDdjiO8hNxE+ANQTpauyG9JJsta72jaknekhhcU22WzsuMo0qPBxLDtPXyceUUGRzdtFAVAH7E7P0XP/26LDGbneLopPaOgX7CmncpVyqvLPSl5hsfq+9oOGTf3RolL5Jre72vqywe+u1MkzBTr0q1w9dWNnbxcHgJfd+t4q5RfZtPVIlr576rqar7AeXuyWrvmoh3+ey/IKi3XdG0slSXteG6xGAc6v3X86lKGM3EJlJGdWuK6GHuKkBlbzcfLnbmLxu455uSQA6oL8n2tBdx7N9nJJ3JNwKMPbRfA6q0/gWWfPd6XOLSi2eOv1T4MKHwDqL1fH+fhs3SE9+ulG5Rf57gkkv5ZvHZ88na+th7MkOTZEras9X+qCbUey9O81B0VTD9c0qNsuqDumLNsnPz/psV9c7O2ioJYcy87TN5tTdWfv9gpv3MjbxbH721fbJEmzNx72cknqrt6v/iBJmjuyv5qXOnZ16rzqzXsXTnbEr99bJUm6q0+7Kt/u6d4uOXmFahZSd75jrqDmow7bcPCU/vhZglIzz3q7KB6VdbZQf1+wSxPn71JOPR0VEtLwj9bp1e92aszsRG8XxanTeUX2f3vqpFqnTs4esHrvCcdapDr0B+5Oz7F0e6XzQmXdkkse+FfpujyYPuJ3pqvHi4s04fudHlunFQgfddidU9fo+61pdfbHu7pK9zYqKq5Dv2Z1SHE9qLvdc+zcaJ91tY2VK+NaNCQV3VGpq3tp2xGL2+nU0VaiL83bIUl6f8V+L5fEPYQPH3A4o37VfJRW8sM2Z2OKpizb59Wy1BVjv9yiK19epOM5+d4uisd9temwNiV7prFk8slcLdqe5rQdQtZZnkHiKaX3L4HtHJq+1FyDDB91NMA2GM5qHJ/57xb9fcEu7T1W+bMxNhw8pQF/X6Ilu9JrVIYvNiRraVLdvCL/fH2KcvKKNHNdsreL4lEJh05p9Bebdcdkz4x/cP0/luqRTxO05OealW1Hsuzzvt+aVuX7a+NR8c7Wc+J0vs+OLUQD0/NcHcTOZvE+89VA2CDDh28eqvqp7I9bVW1Ahn+0ToczzurBGRurvc3d6Tn6y/+2asT0DdVehxXq2xDM+47XzkPJSrqdLtpRs0BaG5JP5qrPqz8o9q3l3i5KtTk0+fDwj6fNZhS3dK/W7Dvp2RXXAsfeVBXvCFf2UV6h53pa2Xwz1zbM8AHvqsk51RNXkOnZeTVehxXqWfaoNf4eSGm11eB00Y5zNTDJVYyK6Ss8HT6+3XpU/1iYpHs/XOvZFdcCT34f/73mkAfX5psaZPjgR73uKPtbVp3fttP5RVUvVAO5BUU1qn7OKyzWit3H3b7aqW81H7X15/hXsWJjjEbN2qSxX27x+Laren5H9tna781VmzW5xtTuCKfJJ2unNsybrL5TZfVtHk9pkOGjNuQWFOnNRUnanppV9cI1cOJ0fq2fbGubJ7uZffnTYXV/YaHeX147jVUPZ+Sq6/iF+v209dVeR5dxC/SHj9frr19udet9PDLcRVXsp8MZZzU3MVWfr0/xaHX3Gwt2qderi50+FyY186z+tWSPJi3Z67HtWaWywddqEsILimzKyvXdrvWlv4916XTvziEpKrZp3/HK29VZpUGHjyW70rXHA33Fs3IL9ep3O/Xekr0aMuncQDO10VArM7dAfV79Qd1fWFjjddlsRvuPn9aZ/CJl5nqvZ0BNd9OY2ZslSRPm7/JAacr78qcjkqRVe0/UfF2bjri1fH3LHrUVpiqr+Zi++oD9eRyeNnnZPmXmFipuafmAcfcHa/TPRbsdppX9TTiek1/lQ8q8oeQzL5WcZD3zWzbwzWW64uVFOpZjzW3PXWnZGv/1Nh3LydP6A6ccGiRXh8M4H5XsEquDiTsNTp+elagb31yuWeu935i9wYaPxJRMPThjo3719gotTTqm+J3uN1bLLyrWa9/t0BUvL3LomTBu7jb1n7jE4VkANVHyQd/hwedPjPt6mwa+uVzdXlioK19eXKPalGKbcevx0ZWdgtwJI3/4uPq1ESWe+nyTfvRAsKgNaVm+0TbF2/wq+USVjIHgjLsXCMYYvbV4txZtr7onTcqp8t3j//qVY83X1a/9oOveWOrx2oDtqVn6v082uj0I10OfbJDNZnS2zHNLPHXbpWTIgB/3WtO49OZ3Vurfaw7pgY836K7319hHIK1tVvcQcmdIoO+2HpUkTa2lmmJ3NNjwUfpBUiOmb9BDn2zUGTdPwA/O2KAPVx4oN/3TtYeUmpWnLzbUfro0xmjr4Sy3q5M/K9ONc381q+KKbUaxby3XoHdWVOtLV5NuYit2H6/2e0t8szlVv/tonVvvSTmVa3/uRW2yor2AVeZtTtVXm2pnOPOq2nxUJCfPve/70qRjmhS/R498mlCt7X2+PsXp9Jo2Ri37598x+Uf9sDNdN729osLB6nILyv/tq/ee1E/JGeVq3Gqzt4sVtxY9edFWwpvdW0/nF+nJzzdp4c8h2EebfDTc8OHMWTdP4KurSPCVXZG5o7Lv5xcbUnTrv1bp99POnUDX7j+pT9e635K6orLO33q00kGh0rLzdODEGe05drrC2pOcvEKHcLS4GrVM3uLsi33dG0t1679W1XqVeX1q8/Hk55uq/L5U1wcr9mv2Bucn9srsP+FeY8e0rIoGfXP9OGXnFSrhUIZHB+76sUw31dI9wv5TwW9BRU/x3X/8TKW1RaUVFtsUt3SvNqdkulbQn9X2iTuvsNit2qSiYptLIwo7fB29eMKfvHSv5m1O1aP2EHy+MMs9cEFmlQYbPpz9XHiiy57DNjx97nDygS+pwdhw8FxAuOeDtRo3d5vb/eadlTUpLUePf/aTfVCozSmZmvD9TocaotI/op+tS9aYLxIdvshn8ovU48VF6vXKYvu0Z/9bqtdBub+pZt/q/KJiPfn5Jv03wf2r7I9XHVD/iUtcDhVJLjzDIbsOPbvGmPJV6vVBTn6Rnv2f+z1ZFrswLshHK/frrUVJklw7aVZV+zf4nZUaNuVHh0HQanrlOuPHgxXOW1LB0PbTVzt/j7P96Fi+n1uBGKM/zd6sfyxM0m1xq10saeUenLGhRk8a3pGarY9XHdCVLy/SFS8vcuk9RcU2XffGUv3q7eVVHjtXLyZru/fJsVIjH284eEonTp9vs3e/i7ei68KFTYMMH4kVJPXqVt+6ojZ6qBQW27S1gkZUyafcu6pz9lk8VKYb3G1xq/X+iv1654fd5ReWNHH+Ln256YjeWHi+8WfJ8z1yKznplQ4rNX3Wy6z1KZq3OVV/nrNZxhj9e83BcjU3Ff2IvPztDh3JPKtXvj1/5VfZd7Sqkh48cUY9X3T8EYxbulefrjko6VwwWbnnuMPfX1h8/qrV0x/HMbM36/LxC7T3mLUP5KpLKnx+SQXTX/1upyYt2auUU7kVLlP6M/KftZXfaj3y80Miv9929Py2K31HzVT0m+bqyWdXWrZOnD5/spu3+Vy5Z29M0TebU+3T8wqL9a8leyqsUXGm2Gb08arzt62X7Drm0NjVXbdMWqmXv92hvMKKxwIqqREpKrbpzUVJ+m/CYR3NytP+42eU78YYQttr4VZOWcey8xzOVQVFNj366UaHC6s7p64p976jWb7xOI4GGT4kqchJNZuzk5LNZrQ7PadGjYhmrU9W9xcWavrq8u1DJGnmumStLtXocf2BU5pbqmeE/XeiTPF+KHPlNmL6+dRbaWtsJzNL/vbjOflOq1FLl31PqSHQnW3n/eX77dvZkVr5l9TI8emiJ8+U73kzbu42/fq9lS41as0o1XNn4fZ0jf96u+6Y/KM2HDylYpvRtFUHtOVIZqXrcPbZkM6FCXf876fytS//WJikcV9vlyTdNXWNfj9tvaat2u8w366C84MxRk9+vkkvu1g9np6dp5RTufrq58/UHXE/eqTLaUGRTSt2H3fafsCdBsi1xd02XJWp7JbspuRMl5ZzYNHuqWlt7vdb0zS8VJuo77YeVdbZQs3e6PjZjlu6V/9ctFuD310p6VwYSTh0yunnoOQ3Y9aG5HLf98qOmSc+U9N/PPc7NnvjYb23ZK+ec6f7e6ld+eI3577Dx3PydXvcaofbfrvTXWs/l19UrG+3pOqUk988Ser7erxuj1ttH75hbuIRLdxedW3dO4v3VLmMs+7hVgv0dgG8xdk9Pr+fo1hWbqHumLJaN3ZprW82pyo9O1+9OzZXQZFN/7zzCl0W1cylbZRcXZR8wF+at0Mj+kc7LJOYkmlvBX9w4hBJ0l3vO6ZZZ8Pnjpz5k65qH+EwbWmSa/f7nD398JZJK/XUwEvs4xJ89cdrlVLqgXal7wMvSzqul+ft0PNDLq90O5OX7XM8mVbEYdhix1n5RcX2Nizjvt5W9bpKKX2Ff+fUNfrHb3s61GpUpHS1aeny3PDPZfZjJMnhirA6Sh69PXdTqh65/mJJ0mcutNdZvfek5v181Tn+1q6y2YyKbEZBgc6vJfq9Hu/w/5z8Ik2cv0sv/qZbTYqvNxbs0kerDugXl16gTx7s6zAvtQ5cfX3ycw1TadU9F5f9XJa+OCh7te9K1bcrt3CKbUYBNayOraiG40QNHlp4xUuLyn3WtpRpgP3gjA36cd9J/fWWLvbPdlnOLkwqumhKz87TLe+u1G97t6teoX9WEmAOuVEzvCk5Qzl5RbqyQ4R9Wkk5/7kwSYkpmRXWppdWtvfRm4t264MV+3VpZFMtGv0Lh3kfrTz/G/3ToQx1axvucJFWGVc+W4V14GniDbbmw5mSr+mMHw9q//Ez+nDlAaVnn/uSJhzK0NYjWRr0zooabaPsPc0jpU7wFT3XpKSqtvRn6rstR/Xqdzsr3E5lH62JFYyJUXpApB/3naz0RP3x6gNatCOt0hqWyU7GPyjLGMcTQtn7paX/W7aHTlkZZwocli+bL5/5b8XtAkp3lV72c4iz2UylX+Sxbg4a5q6Kbg/dN+38leijn27URX/9Xr1eWayth7Nc7p777ZbzVebp2Xn6YUd6hVeWGWcK9M+FSfYeUWv3n9T8rUftobCmjdxy8go1KX6PffCj/cdPa5kHHvrnrH1LZbdObDajcXO3ac7GlJ+XLfNZLPXvUV8kVrhdd/eHMUar9pzQ5GV77dt8a/FuXfHSomr3QivhLLtknS3Umv01a/xb1WMOShrBvrlot1JO5ToEjZILP2e3YktqHY/l5OmTHw/a20xNWbZPJ88UlLtwSsvK0wcr9rne6NWN9Hk6v0h5hcW6Y/KP+sPH65Ve6rtV8jt12kmtX0Vuetvx3PHBz39L6ZqSDQdP6fo3ljr9bXe16L7S+6XB1nw4M39bmi6+oKnDfXdnDmfkql3zxlWuz9lnxWY7VyX58L83amCX1rqgWbB93pJdx3TblRc6XZe7VY5jv9yqIpvR76/p6Nb73FG6oVNZd079UWdcbNxYthF5flGxAv393b7q6//3JXr4uovs/3fn+Qllx2AothkNmbTSXjtR4vXvKw58JWw2o/UHT1XalfPx/5zvrunQlbHUv1Mzz+qGfyzVA9d20gP9o/XiN9vtD1ErUVINezq/SLf+69w4BqVrZypizLn7+ZHNQuw1IxOG9tDQXhfq68RUDbikldpGhEqSRszYoMSUTE1bdUA7X7lZ93xQ9XM4qmpTUFBk04aDp7Ri93H7CeWtxbt1YMItGvjmuYewzXksRld3amF/T9bZQoWHNqpy29W1eGe6PVDd2ad9rW1HcjxBpGXl6fHPfpIkXXxBUw3qFqVJ8eeqzv+5KEmTh/dWenaeZq1P0b1926t1WEi59SWfzFWHluV/k5zddqmNES4rOtz5RecadD5782X2ac/8d4sKi439NmBpf1+wS+GhjTR99QHtOXZa6w+cUtzwXhVu9/m52/SDG73nzt/BrvzzebagWN1fWKhGAeeXs18EqnYesLf5cKbTNhwl3yVXfw3rwB1PlxA+SinphTHyl86rCUsM+PtSPXL9RZUuI0lbDmfqxjeXOUzz8zvX/W3lnhNaueeE/vW7q1wq28ZDGdrr5o/GuLnbyoUPV+/1u9rG5dn/bXY6vaT3TZXbkXE4UeXmF+nKlxYrulUTff/0dW411C17JVWT2yJ7juWUCx7S+auVEmXHGsk4U6DRsxPttScVmb+t/EBV245kOfwNJVenL87boQf6R1faq6G0xJRMXfnzLbmKBro7eaZAN7+z0mHaX7/aqqNZeZoUv0fNQgK19cVB2nvstL1Kuar2DGlZeWrdLFj+/lX3C7j0+flOpx8vdTvgm8RUe/iYFL9Hby3ere4XhunbJ6+rYu3nOCtDQQUXFsbIYaTftKw8RTRuVH4hDyl9/EuChyRN+H6nQ/uvku/GiOkbtONotuJ3peubJwaUW9+iHWny9/NTxzIBxN+iuu2qPu9HMx1r5MqG/YrmVdUt353gIUlFLj4Cdv+Jc7+1pW9PPFDqKdglv481HTW1xKdrD+mFn9uRlFVSAlcbCZeurf107SHl5BXqjzdcUucamhM+nHDlN6bsSciZuYmp5abtPJpd4Qn1+61HK6z5+OTHg/bR6apr/YFT5dqTVMSVffD3BbvcHqhpw8FT5aaV/kolHMrQ2cJi+8BA26tosFqWp04PD83Y6NJyZUdZvWPyah086V5jrpJaLU+NwHh73GodnDhE6dl55dp7VMaY82Gq5LguLDOaZ0XdkBduT9Ojnybo1z3b6F+/6+UQItzRt1R5S7eleGvxuR5W245k66OV+3XrFW2rtf7KBm4rHZmumRCvDi3On8jfW7JHF/5cE1SbDp7M1cFSNXYlNRcl34eybStKbErOdPr74OyEVdudLGvaFqq0giJbuYb1NRG3dJ+eGdTF6by1+0+qb3QLNQ4KrPL3r2T2ITe/6xWpaDwWSfr7/F3q2iaswnBSVklgMubcbURJ+nWPtop9q2ZNBjyNNh9O1Ga11R2Tf6zwg11ZS+bqBo8f953Q24t3K6+w2OXgIbm2D9wNHvlFxRVUK57/d3GZnVP25GeV0lWs7nA3eEhSkgvDYJetQXOFu1eFrqjoOSklAx59u+WoVu894ZGxHzYeytCz/92soZMd1/Xqdzv1OxcewV66V5YrDpc55qVHHv12y1GnDbVrm6tBoaLulSXh5UjmWY2Yvl6r956o9TEenLW1qagHmSv+798bXRpTx1Wz1ifrSyc90R6YvkFdxy/U14lVd/d1txJsfhW/35V9Vk/nF2nYlB9d3ta8zanKLShy+A0/ecZzgdBTGmz42Hw4s8J5lY3o6Qnvxp/vCpWbX7uDPv3uw3V6N36Puoxb4Nb7amMUwrJVryVKX3GWbtuSmVug3W7+6Hh/6Jzase+4e918j2adrdbw8wfLjO1SUffwypRuqV9Tszce1k+lurKWcGV/OLu1VdkAUJPiq+6iaLXsvELd7EIj94o6L8zbnKqUU7l6Zs5mLU06ruEfrdNGJ7WPte3zGj7I7ICb3dwr89yXWx0G6iprdJmBEp0pGdm5Kh+t3K/Yt5Y73Fqzwr+W7HW4dV4XBhUrq8HedqlsMJt1B6z7cpYdUfD5ubXbg8JVtVH74+zzX/ZckFFqWOQrX14sdy31QC+J+iBmwpJqvS+z1P43xlTaqNgX1YUuhu5w1pbC2TNyKuvt8einCcor1cuusl5ynnA4w/PdrNOyrXvIos1Iz/zXeVu20n75z2VVLlPb+7oi76/Yr8nLvP/wuMo02JqPuqqqERKtUhsPxUutoOajdC1LTbttVnRPHO7rP7F6AaYui31rubeLUGOjv6j6xFjajqPZ2u9mzVlNjPpik2Xbqi2uDhRWV5WtuXE27IEVDz6tTIOt+UDlSsY38aQxsxPLTbu2Hp7gqsPTj1X3hFQXxwwpy9XB7lA/1cZvB2pmkZNGu3/531bdfXUHL5TmHGo+YJmj1TyZNQSuPggLAOoDwgcAALAU4QMAAFiK8AEAACxF+AAAAJZqMOEj142nDwIAgNrTYMLHiZz6NVgSAAC+qsGEj7LPDAEAAN7RYMJHZc90AAAA1qm18BEXF6dOnTopJCRE/fr10/r166t+Uy0yhA8AAOqEWgkfX3zxhcaMGaMXXnhBP/30k6644goNGjRIx45576FftfGgNAAA4L5aCR9vvfWWHn74YY0YMUJdu3bV1KlT1bhxY3388ce1sTmXUPEBAEDd4PHwUVBQoISEBMXGxp7fiL+/YmNjtWbNmnLL5+fnKzs72+FVG2jzAQBA3eDx8HHixAkVFxcrMjLSYXpkZKTS0tLKLT9hwgSFh4fbX+3bt/d0kSRJp87Q1RYAgLrA671dxo4dq6ysLPsrJSWlVrZD+AAAoG4I9PQKW7VqpYCAAKWnpztMT09PV1RUVLnlg4ODFRwc7OlilNO8cVCtbwMAAFTN4zUfQUFB6t27t+Lj4+3TbDab4uPjFRMT4+nNuczPz2ubBgAApXi85kOSxowZo/vvv199+vRR37599c477+jMmTMaMWJEbWzOJWQPAADqhloJH3fffbeOHz+u8ePHKy0tTVdeeaUWLFhQrhGqlQL8iR8AANQFtRI+JOmJJ57QE088UVurd9uVHSK8XQQAAKA60NvFKo38G8yfCgBAndZgzsj+/n7q26mFt4sBoJ65qkOEbruyrbeLUS2bx99U6fzrOreyqCSw2qu3d/fq9htM+JCkyff10i8uvUCSdGFEqHp1iNAfYjpq+gNXa/3fblRUWIhu7halTx7sqy0v3qR377lS0x+4Wq/c3t2ht8zbd1/hdP0XX9BErZo6dumddO9VLpUt/k+/0Mpnf1lu+iu3ddPCUdfr84ev0Z7XBmvva4N1U9dI3dLDsdvypw/1dfh/88aNNO7XXTXgEuc/HkN6tNFNXSO17aVBWvfXG/Vg/2in2z84cYiuvbilJOnNO6/QhRGhTtfX/cIwtQ0PUcsm5/7+xkEBWvfXG3Vw4hDNfLif/m9AdNU7wYlPH+qr+U9fp43PxzpM7xfdQs0bN1L/S1o6fV+nlo0lSVPv66V377nSPv3qTs0dltv58s0uleOhMuV/4NpOeuuuc5+DsYO76IVbu2rs4C6VrqNLVDP16Xh++0EB5b9+rZud63beOChAPduFl5s/pEebCtd/V592lW6/In++6dJy06aPuFoB/n7q3bG5kl69WdteGqT7YzpWa/3OTBjaQ5LUrW2YWjcL1tCrLrTPe+DaTuXKNGPE1fbtNw4KsE8f0b+ThvfroFt6RGnXKzcrcfyv9N/HYhQe2kiSdGOX1moWHKjnh1yuTx7sq+hWTRT3u14acEkrPfqLi3Rw4hD97/Fr7esreZ8rDk4cooMTh+irP/bXu/dcpZn/189h3vdPXaf/PuZeD79//LanJOnh66r+vtzbt732vjZY85++Tv2iK7+wCmlU/rP2v8evVXjjRmrf4tx3ev7T12nPa4N1ZfsI9eoQob2vDdanD/XTlhdv0od/6KONz8fqwf7ny7V49PX6rNTf7I6+nVqoVdNgrf/bjfptb8fP7Zt3XqFOLRvrhVu7urXOyLBgvX5Hj2qVp0TXNmHa9/otut3FMPmnX5X/7pRV+jtfWlRYiNPpo2I7a91fb9S3Tw7QvX07OOyjHheGa2ivCzWom/P2k6NiO2vqfb3l53fu9yX2csflXri1q176TTctHn297rvGc9/n6vAzdexxr9nZ2QoPD1dWVpbCwsIs3bYxRn4V9MnNLSjS4h3puuGy1goPbaS3FiVp0pK9uviCJgr099elUc303r1X6XBGrp6fu02P/+JiXd2phfz9/bRyz3EV2YwuatVEg95ZobxCmzaPv0lnC4tVWGxT46AAtWx67qSTdbZQS3al6/P1KXp20GXqU0ltTW5BkQqKbIr4eQyTb7ekavXekxra60JdXep9BUU2jZmdqAGXtNI9fTtUuR8GvrlM+4+f0V9u7qLHb7i43PyVe47rb19t05T7eqlb2/InyIr2pTFGo75IVKumwQoM8FPPCyP0bvxu7U4/LUmaO7K/MnMLFBTgrzkJhzX+113VvIljmCu2GW1KzlD3C8MV4O+nQH8/bU/N1rKkY3r4+osUHBggZ3akZis0KEDRrZpo9d4TeurzTXp9aA8N6halE6fzlZ6dp8nL9mnMry5VUIC/moUEyhjpWE6+/PykSyOb6WxBsUKDApRxpkARjRvJz8+v3N95Or9IGw6eUv+LW+n46XxFhDZSwqEMZZ4t1G+uOP+DlpaVp8iwYH350xH9ac5mSdIbv+2pu/o4H+E3v6hYicmZ6t2xuQID/HXydL6S0nK05UiWJs7fpdmPxqhvdAvlFRYr0N9PgT8Hm9TMs7p24hJJ0oEJt2hp0jE9OGOjJCnud72UX1Ssob3aacqyffp49QEdz8nXC7d21Yj+zk9+3289qj9+9pOG9Wqnf/y2p9Ky8/Tekr0a0b+TQhsFKCO3QJdFNVNw4Ln9NDfxiI5knNVHqw7ozt7t9NSNnSVJ7Vs0LrfuvcdOK6+wWN0vPP+Zyi8qVoDf+b/HVflFxcrJK1Krpq6NIbQ9NUttwkPVokmQsvMK9cOOdHWJClPHlo3l7+enE6fz1b5FY21KztCHK/dr7ODLnf4NXyceUceWTXRl+wj7tMMZufp07SE9cG0nrdt/Su/G79GHf+itkEYB+nx9slo0CVZBkU3XX9rK4fuUcChDB06c0a6j2crILVRII3/df20nFRUbpefk6dqLWzp83j9auV+vfrdTf/rVpXpi4CUqthkF+Pspv8imkEbnjsfp/CKlZOQqM7dQt/wcZPMKi3XqTIHaVnBhUZVpqw7olW93aMaIq3XDZa11JPOsvlifrElL9kqS9r1+i0ra+5eURZJsNiP/KjoCGGNkzLma63Fzt+nTtYfUN7qF4n7XS9NWHdA1F7VQv+iWyskvVERokIIC/ZWUlqO5iUdUWGTTR6sO6P8GRGtA51Y6nV+kpLQc/fGGS3T5+AX2bdzbt71e/E03p5+X3IIiLdyepoGXRarQZlNq5ln1uDBcC7en6bKoMEW3aqKiYpuST+Xqogua6mjWWcVMOPd9692xufp0aq5nbrpMgQH+WrwjXQ//+9x3b9crNyukUYDmbz0qf38/Xd/5Am04eEpbj2Tp8V9cXOV+kaTCYpsyzhRo/cFTGtiltRoHBTrMC/T3k5+fn4ptRlsOZyqicZCiWzWpcr014c75m/BRTYXFNi3ddUx9o1vYT/6uvq+o2Cg0yPlJsi7IySvU5pQsxVzc0pJeQmcLilVQbHPrqrOmKguaVss4U1AuZHnSkcyzCgsJVLOQRjLGOPxwllVywvI0V040qLlTZwrUohY/S97m7ve2qNjmNLgWFts0a0OKru/cSh1bevaEnFtQpLxCm9PjsGrPCTUK8FO/i5zX2Po6wgcAALCUO+fvBtXmAwAAeB/hAwAAWIrwAQAALEX4AAAAliJ8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLBXq7AGWVPGQ3OzvbyyUBAACuKjlvl5zHK1PnwkdOTo4kqX379l4uCQAAcFdOTo7Cw8MrXcbPuBJRLGSz2ZSamqpmzZrJz8/Po+vOzs5W+/btlZKSorCwMI+uG67jONQdHIu6geNQd3Asqs8Yo5ycHLVt21b+/pW36qhzNR/+/v5q165drW4jLCyMD1UdwHGoOzgWdQPHoe7gWFRPVTUeJWhwCgAALEX4AAAAlmpQ4SM4OFgvvPCCgoODvV2UBo3jUHdwLOoGjkPdwbGwRp1rcAoAAOq3BlXzAQAAvI/wAQAALEX4AAAAliJ8AAAASzWY8BEXF6dOnTopJCRE/fr10/r1671dpDprwoQJuvrqq9WsWTO1bt1at99+u5KSkhyWycvL08iRI9WyZUs1bdpUw4YNU3p6usMyycnJGjJkiBo3bqzWrVvrmWeeUVFRkcMyy5YtU69evRQcHKxLLrlEM2bMKFeeqo6dK2WpDyZOnCg/Pz+NGjXKPo3jYJ0jR47ovvvuU8uWLRUaGqoePXpo48aN9vnGGI0fP15t2rRRaGioYmNjtWfPHod1nDp1SsOHD1dYWJgiIiL00EMP6fTp0w7LbNmyRdddd51CQkLUvn17vfHGG+XKMmfOHHXp0kUhISHq0aOHvv/+e4f5rpTFVxUXF2vcuHGKjo5WaGioLr74Yr3yyisOzxPhWPgA0wDMmjXLBAUFmY8//ths377dPPzwwyYiIsKkp6d7u2h10qBBg8z06dPNtm3bTGJiornllltMhw4dzOnTp+3LPPbYY6Z9+/YmPj7ebNy40VxzzTXm2muvtc8vKioy3bt3N7GxsWbTpk3m+++/N61atTJjx461L7N//37TuHFjM2bMGLNjxw7z3nvvmYCAALNgwQL7Mq4cu6rKUh+sX7/edOrUyfTs2dM8/fTT9ukcB2ucOnXKdOzY0TzwwANm3bp1Zv/+/WbhwoVm79699mUmTpxowsPDzdy5c83mzZvNb37zGxMdHW3Onj1rX+bmm282V1xxhVm7dq1ZuXKlueSSS8y9995rn5+VlWUiIyPN8OHDzbZt28znn39uQkNDzfvvv29fZvXq1SYgIMC88cYbZseOHeb55583jRo1Mlu3bnWrLL7qtddeMy1btjTffvutOXDggJkzZ45p2rSpeffdd+3LcCzqvgYRPvr27WtGjhxp/39xcbFp27atmTBhghdL5TuOHTtmJJnly5cbY4zJzMw0jRo1MnPmzLEvs3PnTiPJrFmzxhhjzPfff2/8/f1NWlqafZkpU6aYsLAwk5+fb4wx5tlnnzXdunVz2Nbdd99tBg0aZP9/VcfOlbL4upycHNO5c2ezePFi84tf/MIePjgO1vnLX/5iBgwYUOF8m81moqKizD/+8Q/7tMzMTBMcHGw+//xzY4wxO3bsMJLMhg0b7MvMnz/f+Pn5mSNHjhhjjJk8ebJp3ry5/diUbPuyyy6z//+uu+4yQ4YMcdh+v379zKOPPupyWXzZkCFDzIMPPugwbejQoWb48OHGGI6Fr6j3t10KCgqUkJCg2NhY+zR/f3/FxsZqzZo1XiyZ78jKypIktWjRQpKUkJCgwsJCh33apUsXdejQwb5P16xZox49eigyMtK+zKBBg5Sdna3t27fblym9jpJlStbhyrFzpSy+buTIkRoyZEi5fcVxsM4333yjPn366M4771Tr1q111VVX6cMPP7TPP3DggNLS0hz+/vDwcPXr18/hWERERKhPnz72ZWJjY+Xv769169bZl7n++usVFBRkX2bQoEFKSkpSRkaGfZnKjpcrZfFl1157reLj47V7925J0ubNm7Vq1SoNHjxYEsfCV9S5B8t52okTJ1RcXOzw4ytJkZGR2rVrl5dK5TtsNptGjRql/v37q3v37pKktLQ0BQUFKSIiwmHZyMhIpaWl2Zdxts9L5lW2THZ2ts6ePauMjIwqj50rZfFls2bN0k8//aQNGzaUm8dxsM7+/fs1ZcoUjRkzRn/961+1YcMGPfXUUwoKCtL9999v/xud7aPS+7l169YO8wMDA9WiRQuHZaKjo8uto2Re8+bNKzxepddRVVl82XPPPafs7Gx16dJFAQEBKi4u1muvvabhw4dLcu3v51h4X70PH6iZkSNHatu2bVq1apW3i9LgpKSk6Omnn9bixYsVEhLi7eI0aDabTX369NHrr78uSbrqqqu0bds2TZ06Vffff7+XS9ewzJ49W5999plmzpypbt26KTExUaNGjVLbtm05Fj6k3t92adWqlQICAsq1uk9PT1dUVJSXSuUbnnjiCX377bdaunSp2rVrZ58eFRWlgoICZWZmOixfep9GRUU53ecl8ypbJiwsTKGhoS4dO1fK4qsSEhJ07Ngx9erVS4GBgQoMDNTy5cs1adIkBQYGKjIykuNgkTZt2qhr164O0y6//HIlJydLOr8vq9pHx44dc5hfVFSkU6dOeeR4lZ5fVVl82TPPPKPnnntO99xzj3r06KHf//73Gj16tCZMmCCJY+Er6n34CAoKUu/evRUfH2+fZrPZFB8fr5iYGC+WrO4yxuiJJ57QV199pSVLlpSreuzdu7caNWrksE+TkpKUnJxs36cxMTHaunWrwxd88eLFCgsLs/+Ix8TEOKyjZJmSdbhy7Fwpi6+68cYbtXXrViUmJtpfffr00fDhw+3/5jhYo3///uW6m+/evVsdO3aUJEVHRysqKsrh78/Ozta6descjkVmZqYSEhLsyyxZskQ2m039+vWzL7NixQoVFhbal1m8eLEuu+wyNW/e3L5MZcfLlbL4stzcXPn7O566AgICZLPZJHEsfIa3W7xaYdasWSY4ONjMmDHD7NixwzzyyCMmIiLCoQcAznv88cdNeHi4WbZsmTl69Kj9lZuba1/mscceMx06dDBLliwxGzduNDExMSYmJsY+v6SL50033WQSExPNggULzAUXXOC0i+czzzxjdu7caeLi4px28azq2FVVlvqkdG8XYzgOVlm/fr0JDAw0r732mtmzZ4/57LPPTOPGjc1//vMf+zITJ040ERER5uuvvzZbtmwxt912m9PunVdddZVZt26dWbVqlencubND987MzEwTGRlpfv/735tt27aZWbNmmcaNG5fr3hkYGGj++c9/mp07d5oXXnjBaffOqsriq+6//35z4YUX2rvafvnll6ZVq1bm2WeftS/Dsaj7GkT4MMaY9957z3To0MEEBQWZvn37mrVr13q7SHWWJKev6dOn25c5e/as+eMf/2iaN29uGjdubO644w5z9OhRh/UcPHjQDB482ISGhppWrVqZP/3pT6awsNBhmaVLl5orr7zSBAUFmYsuushhGyWqOnaulKW+KBs+OA7WmTdvnunevbsJDg42Xbp0MR988IHDfJvNZsaNG2ciIyNNcHCwufHGG01SUpLDMidPnjT33nuvadq0qQkLCzMjRowwOTk5Dsts3rzZDBgwwAQHB5sLL7zQTJw4sVxZZs+ebS699FITFBRkunXrZr777ju3y+KrsrOzzdNPP206dOhgQkJCzEUXXWT+9re/OXSJ5VjUfX7GlBoWDgAAoJbV+zYfAACgbiF8AAAASxE+AACApQgfAADAUoQPAABgKcIHAACwFOEDAABYivABAAAsRfgAAACWInwAAABLET4AAIClCB8AAMBS/w+GNaTXqawpvwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b73c37-fe5a-45ae-b960-1f32027625a0",
   "metadata": {},
   "source": [
    "If you want to load them all into a nice electrode grid see the dataloader code below for that. We can also see what kind of data we have access to in our word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6ad469e-7392-4a77-8f79-fe3c0dfc9e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_without_punctuation</th>\n",
       "      <th>index</th>\n",
       "      <th>datum_word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>speaker</th>\n",
       "      <th>is_nonword</th>\n",
       "      <th>cloze</th>\n",
       "      <th>...</th>\n",
       "      <th>in_bert-large-cased</th>\n",
       "      <th>in_roberta-base</th>\n",
       "      <th>in_roberta-large</th>\n",
       "      <th>top1_pred</th>\n",
       "      <th>top1_pred_prob</th>\n",
       "      <th>true_pred_prob</th>\n",
       "      <th>true_pred_rank</th>\n",
       "      <th>surprise</th>\n",
       "      <th>entropy</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So</td>\n",
       "      <td>So</td>\n",
       "      <td>0.0</td>\n",
       "      <td>So</td>\n",
       "      <td>356.1457</td>\n",
       "      <td>427.8257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0.194265</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.020285</td>\n",
       "      <td>8.105922</td>\n",
       "      <td>[-0.13344508409500122, -4.483438491821289, -2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>there's</td>\n",
       "      <td>there's</td>\n",
       "      <td>1.0</td>\n",
       "      <td>there's</td>\n",
       "      <td>438.0657</td>\n",
       "      <td>519.9857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>,</td>\n",
       "      <td>0.172101</td>\n",
       "      <td>0.020413</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.114605</td>\n",
       "      <td>6.571477</td>\n",
       "      <td>[3.2388994693756104, -1.0490363836288452, -1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>there's</td>\n",
       "      <td>there's</td>\n",
       "      <td>1.0</td>\n",
       "      <td>there's</td>\n",
       "      <td>438.0657</td>\n",
       "      <td>519.9857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>'s</td>\n",
       "      <td>0.343256</td>\n",
       "      <td>0.343256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.529522</td>\n",
       "      <td>3.673014</td>\n",
       "      <td>[-0.3139915466308594, -2.5767409801483154, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "      <td>2.0</td>\n",
       "      <td>some</td>\n",
       "      <td>509.7457</td>\n",
       "      <td>591.6657</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>0.269538</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.141145</td>\n",
       "      <td>5.795166</td>\n",
       "      <td>[-0.774554431438446, -3.086632490158081, -1.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>places</td>\n",
       "      <td>places</td>\n",
       "      <td>3.0</td>\n",
       "      <td>places</td>\n",
       "      <td>601.9057</td>\n",
       "      <td>740.1457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>sort</td>\n",
       "      <td>0.033894</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>766.0</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>9.948091</td>\n",
       "      <td>[-1.698758840560913, -3.107893943786621, -4.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6128</th>\n",
       "      <td>it's</td>\n",
       "      <td>it's</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>it's</td>\n",
       "      <td>914626.0608</td>\n",
       "      <td>914677.2608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>'s</td>\n",
       "      <td>0.522330</td>\n",
       "      <td>0.522330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.489406</td>\n",
       "      <td>3.837202</td>\n",
       "      <td>[-3.0750534534454346, -3.043199062347412, 4.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>5109.0</td>\n",
       "      <td>a</td>\n",
       "      <td>914697.7408</td>\n",
       "      <td>914707.9808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>not</td>\n",
       "      <td>0.161952</td>\n",
       "      <td>0.074484</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.279085</td>\n",
       "      <td>6.727214</td>\n",
       "      <td>[1.5721521377563477, 0.7583765387535095, 0.252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6130</th>\n",
       "      <td>uniquely</td>\n",
       "      <td>uniquely</td>\n",
       "      <td>5110.0</td>\n",
       "      <td>uniquely</td>\n",
       "      <td>914748.9408</td>\n",
       "      <td>914938.3808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>very</td>\n",
       "      <td>0.062961</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>8.915740</td>\n",
       "      <td>[-0.6058820486068726, 2.3530914783477783, 2.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>5111.0</td>\n",
       "      <td>human</td>\n",
       "      <td>914969.1008</td>\n",
       "      <td>915097.1008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>human</td>\n",
       "      <td>0.424989</td>\n",
       "      <td>0.424989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.524650</td>\n",
       "      <td>5.993249</td>\n",
       "      <td>[6.587402820587158, 3.085411548614502, -1.1317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>characteristic</td>\n",
       "      <td>characteristic</td>\n",
       "      <td>5112.0</td>\n",
       "      <td>characteristic</td>\n",
       "      <td>915127.8208</td>\n",
       "      <td>915430.2208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Speaker2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>thing</td>\n",
       "      <td>0.185792</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.039803</td>\n",
       "      <td>6.885787</td>\n",
       "      <td>[-1.3647840023040771, -0.5388081073760986, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6023 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word word_without_punctuation   index      datum_word  \\\n",
       "8                 So                       So     0.0              So   \n",
       "9            there's                  there's     1.0         there's   \n",
       "10           there's                  there's     1.0         there's   \n",
       "11              some                     some     2.0            some   \n",
       "12            places                   places     3.0          places   \n",
       "...              ...                      ...     ...             ...   \n",
       "6128            it's                     it's  5108.0            it's   \n",
       "6129               a                        a  5109.0               a   \n",
       "6130        uniquely                 uniquely  5110.0        uniquely   \n",
       "6131           human                    human  5111.0           human   \n",
       "6132  characteristic           characteristic  5112.0  characteristic   \n",
       "\n",
       "            onset       offset  accuracy   speaker is_nonword  cloze  ...  \\\n",
       "8        356.1457     427.8257       1.0  Speaker2      False   0.04  ...   \n",
       "9        438.0657     519.9857       1.0  Speaker2      False   0.02  ...   \n",
       "10       438.0657     519.9857       1.0  Speaker2      False   0.02  ...   \n",
       "11       509.7457     591.6657       1.0  Speaker2      False   0.02  ...   \n",
       "12       601.9057     740.1457       1.0  Speaker2      False   0.04  ...   \n",
       "...           ...          ...       ...       ...        ...    ...  ...   \n",
       "6128  914626.0608  914677.2608       1.0  Speaker2      False   0.02  ...   \n",
       "6129  914697.7408  914707.9808       1.0  Speaker2      False   0.06  ...   \n",
       "6130  914748.9408  914938.3808       1.0  Speaker2      False   0.02  ...   \n",
       "6131  914969.1008  915097.1008       1.0  Speaker2      False   0.26  ...   \n",
       "6132  915127.8208  915430.2208       1.0  Speaker2      False   0.04  ...   \n",
       "\n",
       "      in_bert-large-cased  in_roberta-base  in_roberta-large  top1_pred  \\\n",
       "8                   False             True              True         \\n   \n",
       "9                   False             True              True          ,   \n",
       "10                  False             True              True         's   \n",
       "11                  False             True              True          a   \n",
       "12                  False             True              True       sort   \n",
       "...                   ...              ...               ...        ...   \n",
       "6128                False             True              True         's   \n",
       "6129                False             True              True        not   \n",
       "6130                False             True              True       very   \n",
       "6131                False             True              True      human   \n",
       "6132                False             True              True      thing   \n",
       "\n",
       "      top1_pred_prob  true_pred_prob  true_pred_rank  surprise   entropy  \\\n",
       "8           0.194265        0.002317            45.0  0.020285  8.105922   \n",
       "9           0.172101        0.020413            11.0  0.114605  6.571477   \n",
       "10          0.343256        0.343256             1.0  0.529522  3.673014   \n",
       "11          0.269538        0.027120             7.0  0.141145  5.795166   \n",
       "12          0.033894        0.000159           766.0  0.002006  9.948091   \n",
       "...              ...             ...             ...       ...       ...   \n",
       "6128        0.522330        0.522330             1.0  0.489406  3.837202   \n",
       "6129        0.161952        0.074484             3.0  0.279085  6.727214   \n",
       "6130        0.062961        0.000686           184.0  0.007208  8.915740   \n",
       "6131        0.424989        0.424989             1.0  0.524650  5.993249   \n",
       "6132        0.185792        0.005257            30.0  0.039803  6.885787   \n",
       "\n",
       "                                             embeddings  \n",
       "8     [-0.13344508409500122, -4.483438491821289, -2....  \n",
       "9     [3.2388994693756104, -1.0490363836288452, -1.6...  \n",
       "10    [-0.3139915466308594, -2.5767409801483154, -0....  \n",
       "11    [-0.774554431438446, -3.086632490158081, -1.34...  \n",
       "12    [-1.698758840560913, -3.107893943786621, -4.48...  \n",
       "...                                                 ...  \n",
       "6128  [-3.0750534534454346, -3.043199062347412, 4.13...  \n",
       "6129  [1.5721521377563477, 0.7583765387535095, 0.252...  \n",
       "6130  [-0.6058820486068726, 2.3530914783477783, 2.08...  \n",
       "6131  [6.587402820587158, 3.085411548614502, -1.1317...  \n",
       "6132  [-1.3647840023040771, -0.5388081073760986, -1....  \n",
       "\n",
       "[6023 rows x 62 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data = pd.read_csv('../word-embeddings/gpt2-layer-8-emb.pkl', index_col=0)\n",
    "# Convert embeddings from string to array. Index out \"[]\" from string.\n",
    "conversation_data.loc[:, \"embeddings\"] = conversation_data.loc[\n",
    "    :, \"embeddings\"\n",
    "].apply(lambda x: np.fromstring(x[1:-1], sep=\", \"))\n",
    "# Limit only to words with onset time data.\n",
    "conversation_data = conversation_data.loc[\n",
    "    conversation_data[\"onset\"].dropna().index\n",
    "]\n",
    "conversation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a7e17d5-09b4-461d-9064-d25566c9386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['word', 'word_without_punctuation', 'index', 'datum_word', 'onset',\n",
       "       'offset', 'accuracy', 'speaker', 'is_nonword', 'cloze',\n",
       "       'sentence_signal_length', 'sentence', 'num_words', 'sentence_idx',\n",
       "       'adjusted_onset', 'adjusted_offset', 'conversation_id',\n",
       "       'conversation_name', 'production', 'word_freq_overall',\n",
       "       'word_freq_phase', 'lemmatized_word', 'stemmed_word', 'in_glove',\n",
       "       'token', 'token2word', 'token_id', 'gpt2_token_is_root', 'token_idx',\n",
       "       'in_gpt2', 'in_gpt2-large', 'in_gpt2-xl', 'in_gpt-neo-125M',\n",
       "       'in_gpt-neo-1.3B', 'in_gpt-neo-2.7B', 'in_gpt-neox-20b', 'in_opt-125m',\n",
       "       'in_opt-350m', 'in_opt-1.3b', 'in_opt-2.7b', 'in_opt-6.7b',\n",
       "       'in_opt-30b', 'in_blenderbot_small-90M', 'in_blenderbot-3B',\n",
       "       'in_whisper-tiny.en', 'in_whisper-base', 'in_whisper-small',\n",
       "       'in_whisper-medium', 'in_whisper-large', 'in_bert-base-uncased',\n",
       "       'in_bert-large-uncased', 'in_bert-base-cased', 'in_bert-large-cased',\n",
       "       'in_roberta-base', 'in_roberta-large', 'top1_pred', 'top1_pred_prob',\n",
       "       'true_pred_prob', 'true_pred_rank', 'surprise', 'entropy',\n",
       "       'embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c15dd19-f8dc-4052-a260-be8997afd9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>token</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>So</td>\n",
       "      <td>Ä So</td>\n",
       "      <td>356.1457</td>\n",
       "      <td>427.8257</td>\n",
       "      <td>[-0.13344508409500122, -4.483438491821289, -2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>there's</td>\n",
       "      <td>Ä there</td>\n",
       "      <td>438.0657</td>\n",
       "      <td>519.9857</td>\n",
       "      <td>[3.2388994693756104, -1.0490363836288452, -1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>there's</td>\n",
       "      <td>'s</td>\n",
       "      <td>438.0657</td>\n",
       "      <td>519.9857</td>\n",
       "      <td>[-0.3139915466308594, -2.5767409801483154, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>some</td>\n",
       "      <td>Ä some</td>\n",
       "      <td>509.7457</td>\n",
       "      <td>591.6657</td>\n",
       "      <td>[-0.774554431438446, -3.086632490158081, -1.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>places</td>\n",
       "      <td>Ä places</td>\n",
       "      <td>601.9057</td>\n",
       "      <td>740.1457</td>\n",
       "      <td>[-1.698758840560913, -3.107893943786621, -4.48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6128</th>\n",
       "      <td>it's</td>\n",
       "      <td>'s</td>\n",
       "      <td>914626.0608</td>\n",
       "      <td>914677.2608</td>\n",
       "      <td>[-3.0750534534454346, -3.043199062347412, 4.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>a</td>\n",
       "      <td>Ä a</td>\n",
       "      <td>914697.7408</td>\n",
       "      <td>914707.9808</td>\n",
       "      <td>[1.5721521377563477, 0.7583765387535095, 0.252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6130</th>\n",
       "      <td>uniquely</td>\n",
       "      <td>Ä uniquely</td>\n",
       "      <td>914748.9408</td>\n",
       "      <td>914938.3808</td>\n",
       "      <td>[-0.6058820486068726, 2.3530914783477783, 2.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>human</td>\n",
       "      <td>Ä human</td>\n",
       "      <td>914969.1008</td>\n",
       "      <td>915097.1008</td>\n",
       "      <td>[6.587402820587158, 3.085411548614502, -1.1317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6132</th>\n",
       "      <td>characteristic</td>\n",
       "      <td>Ä characteristic</td>\n",
       "      <td>915127.8208</td>\n",
       "      <td>915430.2208</td>\n",
       "      <td>[-1.3647840023040771, -0.5388081073760986, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6023 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word            token        onset       offset  \\\n",
       "8                 So              Ä So     356.1457     427.8257   \n",
       "9            there's           Ä there     438.0657     519.9857   \n",
       "10           there's               's     438.0657     519.9857   \n",
       "11              some            Ä some     509.7457     591.6657   \n",
       "12            places          Ä places     601.9057     740.1457   \n",
       "...              ...              ...          ...          ...   \n",
       "6128            it's               's  914626.0608  914677.2608   \n",
       "6129               a               Ä a  914697.7408  914707.9808   \n",
       "6130        uniquely        Ä uniquely  914748.9408  914938.3808   \n",
       "6131           human           Ä human  914969.1008  915097.1008   \n",
       "6132  characteristic  Ä characteristic  915127.8208  915430.2208   \n",
       "\n",
       "                                             embeddings  \n",
       "8     [-0.13344508409500122, -4.483438491821289, -2....  \n",
       "9     [3.2388994693756104, -1.0490363836288452, -1.6...  \n",
       "10    [-0.3139915466308594, -2.5767409801483154, -0....  \n",
       "11    [-0.774554431438446, -3.086632490158081, -1.34...  \n",
       "12    [-1.698758840560913, -3.107893943786621, -4.48...  \n",
       "...                                                 ...  \n",
       "6128  [-3.0750534534454346, -3.043199062347412, 4.13...  \n",
       "6129  [1.5721521377563477, 0.7583765387535095, 0.252...  \n",
       "6130  [-0.6058820486068726, 2.3530914783477783, 2.08...  \n",
       "6131  [6.587402820587158, 3.085411548614502, -1.1317...  \n",
       "6132  [-1.3647840023040771, -0.5388081073760986, -1....  \n",
       "\n",
       "[6023 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data[['word', 'token', 'onset', 'offset', 'embeddings']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1aa6a1-3943-44ec-9f6c-4ac504564740",
   "metadata": {},
   "source": [
    "These are likely the primary columns of interest. Notice that there can be multiple rows per word if the word is broken up into multiple tokens. There is actually a bug in the current dataloader relating to this because we don't do anything to handle this duplication so you could try to fix this too if you want!\n",
    "\n",
    "Feel free to use whatever columns you are interested in. You can corrolaries for most of them in this [paper](https://hassonlab.princeton.edu/sites/g/files/toruqf3591/files/documents/Goldstein_et_al_NN_2022.pdf) mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a99e6af-2769-43fc-b60b-2375128faf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Speaker2'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_data['speaker'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5abdef-4cda-40c6-961d-23ff01c64bdc",
   "metadata": {},
   "source": [
    "All of these are heard words, not spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39127345-0f23-408c-b53c-6ee24d321052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.True_, np.True_)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conversation_data['onset'] == conversation_data['adjusted_onset']).all(), (conversation_data['offset'] == conversation_data['adjusted_offset']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad060e0-3a84-4036-a398-64011cd86b58",
   "metadata": {},
   "source": [
    "Don't worry about differences between onset and adjusted_onset they're all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de8f989-f0d2-4800-885c-ffde83e7b75b",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd144c5-6cbd-42f8-b8c8-016356c6dc60",
   "metadata": {},
   "source": [
    "Whatever you implement for a downstream task will likely need to update this code so I'm including it here for ease of access. Most changes will involve changing how we gather this data and the associated label (i.e. if you want to do the \"word present\" classifier you would change the word embedding label to a 1 or 0 depending on whether a word is present in that sample or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a33771b-5b82-4658-9479-783513502ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from utils import preprocess_neural_data\n",
    "from downstream_tasks.encoding_decoding.config import EncodingDecodingDataConfig\n",
    "\n",
    "class EncodingDecodingDataset:\n",
    "    def __init__(self, config: EncodingDecodingDataConfig):\n",
    "        self.config = config\n",
    "        self.encoding_neural_data_folder = config.encoding_neural_data_folder\n",
    "        self.electrode_glob_path = config.electrode_glob_path\n",
    "        self.fs = config.original_fs\n",
    "        self.lag = config.lag\n",
    "        self.new_fs = config.new_fs\n",
    "        self.sample_secs = config.sample_length\n",
    "        # Track which electrodes are accessible and which are padding\n",
    "        self.padding_mask = None\n",
    "\n",
    "        self.signal = self._load_grid_data()\n",
    "        conversation_data = pd.read_csv(config.conversation_data_df_path, index_col=0)\n",
    "        # Convert embeddings from string to array. Index out \"[]\" from string.\n",
    "        conversation_data.loc[:, \"embeddings\"] = conversation_data.loc[\n",
    "            :, \"embeddings\"\n",
    "        ].apply(lambda x: np.fromstring(x[1:-1], sep=\", \"))\n",
    "\n",
    "        # Limit only to words with onset time data.\n",
    "        self.timed_word_data = conversation_data.loc[\n",
    "            conversation_data[\"onset\"].dropna().index\n",
    "        ]\n",
    "\n",
    "        # since we take sample_length sec samples, the number of samples we can stream from our dataset is determined by the duration of the chunk in sec divided by sample_length.\n",
    "        # Optionally can configure max_samples directly as well.\n",
    "        self.max_samples = self.signal.shape[1] / self.fs / config.sample_length\n",
    "\n",
    "        self.index = 0\n",
    "\n",
    "    def _load_grid_data(self):\n",
    "        grid_data = []\n",
    "        # TODO: Test padding mask\n",
    "        self.padding_mask = torch.zeros(8, 8, dtype=bool)\n",
    "        # Used to ensure all electrode data is of the same length.\n",
    "        expected_len = 0\n",
    "        for i in range(64):\n",
    "            curr_electrode_glob_path = self.electrode_glob_path.format(\n",
    "                elec_id=str(i + 1)\n",
    "            )\n",
    "            final_glob_path = os.path.join(\n",
    "                self.encoding_neural_data_folder, curr_electrode_glob_path\n",
    "            )\n",
    "            electrode_file = glob.glob(final_glob_path)\n",
    "\n",
    "            if len(electrode_file) > 1:\n",
    "                raise ValueError(\n",
    "                    \"There can only be one matching file associated with electrode {}. Got {} files matching {}.\".format(\n",
    "                        i + 1, len(electrode_file), final_glob_path\n",
    "                    )\n",
    "                )\n",
    "            elif len(electrode_file) == 0:\n",
    "                print(\n",
    "                    \"No files found for electrode {}. Got 0 files matching {}.\".format(\n",
    "                        i + 1, final_glob_path\n",
    "                    )\n",
    "                )\n",
    "                # Append None to be padded with 0's later.\n",
    "                grid_data.append(None)\n",
    "            else:\n",
    "                data = scipy.io.loadmat(electrode_file[0])[\"p1st\"].flatten()\n",
    "\n",
    "                if not expected_len:\n",
    "                    expected_len = data.size\n",
    "                else:\n",
    "                    if data.size != expected_len:\n",
    "                        raise ValueError(\n",
    "                            \"Data size does not match for electrode {} at path: {}. Expected size: {}. Actual size: {}\".format(\n",
    "                                i + 1, final_glob_path, expected_len, data.size\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                grid_data.append(data)\n",
    "                # Update padding_mask to include this electrode as valid.\n",
    "                self.padding_mask[i // 8, i % 8] = True\n",
    "        padded_data = []\n",
    "        for data in grid_data:\n",
    "            # Pad zero's for held out data.\n",
    "            if data is None:\n",
    "                padded_data.append(np.zeros((expected_len)))\n",
    "            else:\n",
    "                padded_data.append(data)\n",
    "\n",
    "        return np.array(padded_data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through dataset for encoding task.\n",
    "\n",
    "        Yields:\n",
    "            tuple[np.array, np.array]: (word embedding, neural data)\n",
    "        \"\"\"\n",
    "        while self.index < self.timed_word_data.shape[0]:\n",
    "            word_data = self.timed_word_data.iloc[self.index]\n",
    "\n",
    "            # TODO: Add configurable way to filter out examples which require padding if we want to train\n",
    "            # without them. Same for VideoMAE dataloader.\n",
    "            lag_start_time = word_data.loc[\"onset\"] + self.lag\n",
    "            lag_start_sample = int(lag_start_time / 1000 * self.fs)\n",
    "            # TODO: Change to ms.\n",
    "            lag_end_sample = lag_start_sample + self.fs * self.sample_secs\n",
    "\n",
    "            # If we are gathering a sample from before the start of the signal or ends after the signal continue.\n",
    "            if lag_start_sample < 0 or lag_end_sample > self.signal.shape[1]:\n",
    "                self.index += 1\n",
    "                continue\n",
    "\n",
    "            curr_sample = self.signal[:, lag_start_sample:lag_end_sample]\n",
    "\n",
    "            preprocessed_signal = preprocess_neural_data(\n",
    "                curr_sample,\n",
    "                self.fs,\n",
    "                self.new_fs,\n",
    "                self.sample_secs,\n",
    "            )\n",
    "\n",
    "            yield word_data.loc[\"embeddings\"], preprocessed_signal\n",
    "\n",
    "            self.index += 1\n",
    "\n",
    "        if self.index >= self.timed_word_data.shape[0]:\n",
    "            self.index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed730b45-1392-4026-b858-0c591abad55d",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99462f8b-2b05-4318-99e6-ca2ae76d6279",
   "metadata": {},
   "source": [
    "These functions may be generally useful for implementing new tasks, especially the one for generating the dataset which processes the neural data into neural embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5311e4-5fe3-498f-9b8f-370d8699e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, replace\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_correlation_metrics(groundtruth, predicted):\n",
    "    \"\"\"\n",
    "    Calculate summary correlation metrics for two sets of embeddings.\n",
    "\n",
    "    Args:\n",
    "    groundtruth (np.array): shape [num_embeddings, embedding_dim]\n",
    "    predicted (np.array): shape [num_embeddings, embedding_dim]\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing various correlation metrics\n",
    "    \"\"\"\n",
    "    # Flatten the matrices\n",
    "    gt_flat = groundtruth.flatten()\n",
    "    pred_flat = predicted.flatten()\n",
    "\n",
    "    # Calculate overall Pearson correlation\n",
    "    overall_corr, overall_p = stats.pearsonr(gt_flat, pred_flat)\n",
    "\n",
    "    # Calculate per-dimension correlations\n",
    "    dim_corrs = np.array([stats.pearsonr(groundtruth[:, i], predicted[:, i])[0] \n",
    "                          for i in range(groundtruth.shape[1])])\n",
    "\n",
    "    # Calculate per-embedding correlations\n",
    "    emb_corrs = np.array([stats.pearsonr(groundtruth[i, :], predicted[i, :])[0] \n",
    "                          for i in range(groundtruth.shape[0])])\n",
    "\n",
    "    return {\n",
    "        \"overall_correlation\": overall_corr,\n",
    "        \"overall_p_value\": overall_p,\n",
    "        \"dimension_correlation\": dim_corrs,\n",
    "        \"embedding_correlation\": emb_corrs,\n",
    "        \"mean_dimension_correlation\": np.mean(dim_corrs),\n",
    "        \"mean_embedding_correlation\": np.mean(emb_corrs),\n",
    "        \"median_dimension_correlation\": np.median(dim_corrs),\n",
    "        \"median_embedding_correlation\": np.median(emb_corrs)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embedding_dataset(\n",
    "    dataset: EncodingDecodingDataset, model: nn.Module, embedding_batch_size: int, device: str\n",
    ") -> tuple[np.array, np.array]:\n",
    "    \"\"\"Gathers word embeddings and generates neural embeddings using model from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (EncodingDecodingDataset): dataset used to gather word and neural data.\n",
    "        model (nn.Module): model used to generate neural embeddings. Expected as of now to output embeddings of shape\n",
    "            [batch_size, num_tokens, output_dim] where num_tokens is the number of patches for the VideoMAE model, although different\n",
    "            models could be plugged in here as well. Embeddings are joined together using average pooling to form one summary embedding.\n",
    "        embedding_batch_size (int): The number of neural examples to pass into the model per-inference. Can speed up inference by\n",
    "            parallelizing at the cost of RAM or VRAM.\n",
    "        device (str): The name of the device to run inference on. Model is assumed to already be on this device.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, np.array]: (word_embeddings, neural_embeddings) both parallel arrays containing the embeddings for our examples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    print(\"padding mask:\", dataset.padding_mask)\n",
    "    model.initialize_mask(dataset.padding_mask.to(device))\n",
    "\n",
    "    # Setup dataloader and iterate through examples.\n",
    "    word_embeddings = []\n",
    "    neural_embeddings = []\n",
    "    \n",
    "    def _generate_neural_embeddings(neural_batch: list):\n",
    "        neural_data = torch.cat(neural_batch)\n",
    "        neural_data = neural_data.to(device)\n",
    "\n",
    "        # Model output is shape:\n",
    "        # [batch_size, output_dim]\n",
    "        # Does average pooling on data.\n",
    "        model_outputs = model(neural_data, forward_features=True)\n",
    "\n",
    "        for embedding in model_outputs:\n",
    "            neural_embeddings.append(embedding.detach().cpu().numpy())\n",
    "\n",
    "    # Collect data into batches to accelerate inference.\n",
    "    neural_batch = []\n",
    "        \n",
    "    for word_embedding, neural_data in dataset:\n",
    "        word_embeddings.append(word_embedding)\n",
    "        batch_ready_neural_data = np.expand_dims(neural_data, 0)\n",
    "        neural_batch.append(torch.from_numpy(batch_ready_neural_data))\n",
    "\n",
    "        if len(neural_batch) == embedding_batch_size:\n",
    "            _generate_neural_embeddings(neural_batch)\n",
    "            neural_batch = []\n",
    "            \n",
    "    # Catch any remaining examples.\n",
    "    if len(neural_batch) > 0:\n",
    "        _generate_neural_embeddings(neural_batch)\n",
    "        neural_batch = []\n",
    "\n",
    "    word_embeddings = np.array(word_embeddings)\n",
    "    neural_embeddings = np.array(neural_embeddings)\n",
    "\n",
    "    model.initialize_mask(None)\n",
    "    model.train()\n",
    "\n",
    "    return word_embeddings, neural_embeddings\n",
    "\n",
    "\n",
    "def merge_data_configs(\n",
    "    encoding_data_config: EncodingDecodingDataConfig, ecog_data_config: ECoGDataConfig\n",
    ") -> EncodingDecodingDataConfig:\n",
    "    \"\"\"Overwrites fields in encoding_data_config with the fields set in ecog_data_config.\n",
    "\n",
    "    Args:\n",
    "        encoding_data_config (EncodingDecodingDataConfig): encoding data config which will have fields overwritten.\n",
    "        ecog_data_config (ECoGDataConfig): ecog data config which contains field to overwrite in encoding_data_config\n",
    "\n",
    "    Returns:\n",
    "        EncodingDecodingDataConfig: config with ECoGDataConfig fields overwritten other than original_fs\n",
    "    \"\"\"\n",
    "    # Maintain original_fs from encoding_config because it could be different than the one for pretraining.\n",
    "    encoding_original_fs = encoding_data_config.original_fs\n",
    "    updated_config = replace(encoding_data_config, **asdict(ecog_data_config))\n",
    "    updated_config.original_fs = encoding_original_fs\n",
    "    return updated_config\n",
    "\n",
    "\n",
    "def run_regression(X: np.array, Y: np.array, num_folds: int) -> np.array:\n",
    "    \"\"\"Builds a linear regression model from X->Y using num_folds folds.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Shape [num_examples, num_variables] the independent variable.\n",
    "        Y (np.array): Shape [num_examples, num_variables] the dependent variable.\n",
    "        num_folds (int): Number of folds to run training/testing over.\n",
    "\n",
    "    Returns:\n",
    "        predicted_values: np.array of shape [num_examples, num_variables] where the ith row corresponds to a prediction of\n",
    "            the ith in Y, generated when the fold containing the ith row was in the test set.\n",
    "    \"\"\"\n",
    "    # Make folds of data.\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    # Used to track predictions for\n",
    "    all_predictions = np.zeros_like(Y)\n",
    "\n",
    "    # TODO: Allow for adding a torch linear head to model which can be used to pass gradients backwards and finetune the\n",
    "    # model if that's something we choose to do in the future. Using sklearn for now because it's simple and easily\n",
    "    # runs on cpu. Can also allow for non-linearities in torch model if necessary.\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_x, test_x = X[train_index], X[test_index]\n",
    "        train_y = Y[train_index]\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        predictions = model.predict(test_x)\n",
    "\n",
    "        all_predictions[test_index, :] = predictions\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93501bb9-4fb7-4f98-bc48-613327549e4f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba19da-7132-4eef-9716-4642d98c0216",
   "metadata": {},
   "source": [
    "Current config setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbc40c89-6cec-4ac5-b03e-b43d6fbb3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downstream_tasks.encoding_decoding.config import EncodingDecodingExperimentConfig, EncodingDecodingTaskConfig, EncodingDecodingDataConfig\n",
    "from downstream_tasks.encoding_decoding.utils import run_encoding_task, run_decoding_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02905695-c8d9-496f-b49d-4fbf51735a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_experiment_config = EncodingDecodingExperimentConfig(\n",
    "    encoding_data_config = EncodingDecodingDataConfig(\n",
    "        conversation_data_df_path = os.path.join(path_to_github_repo, \"word-embeddings/gpt2-layer-8-emb.pkl\"),\n",
    "        encoding_neural_data_folder = os.path.join(path_to_github_repo, \"preprocessed-highgamma\"),\n",
    "        electrode_glob_path = \"NY*_*_Part*_conversation*_electrode_preprocess_file_{elec_id}.mat\",\n",
    "        lag = 0\n",
    "    ),\n",
    "    encoding_task_config = EncodingDecodingTaskConfig(\n",
    "        model_path = \"\", # Unused here.\n",
    "        embedding_device = device,\n",
    "        embedding_batch_size = 8,\n",
    "        num_folds = 2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb5a3e-9470-4160-bc4e-698a8eed0f9b",
   "metadata": {
    "id": "KPykdufdulfI"
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025dc9a-161c-4159-8c3d-a6fe638a4f21",
   "metadata": {},
   "source": [
    "This is our encoding task (word-embeddings -> neural embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c7f2c41-25a0-4241-9bc8-8060b45cd1ab",
   "metadata": {
    "id": "Eq5gibh5JSQy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for electrode 9. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_9.mat.\n",
      "No files found for electrode 59. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_59.mat.\n",
      "No files found for electrode 60. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_60.mat.\n",
      "No files found for electrode 61. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_61.mat.\n",
      "padding mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "encoding_data_config = merge_data_configs(\n",
    "    encoding_experiment_config.encoding_data_config, ecog_config.ecog_data_config\n",
    ")\n",
    "\n",
    "dataset = EncodingDecodingDataset(encoding_data_config)\n",
    "\n",
    "word_embeddings, neural_embeddings = generate_embedding_dataset(\n",
    "    dataset,\n",
    "    model,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_batch_size,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_device,\n",
    ")\n",
    "\n",
    "predictions = run_regression(\n",
    "    word_embeddings,\n",
    "    neural_embeddings,\n",
    "    encoding_experiment_config.encoding_task_config.num_folds,\n",
    ")\n",
    "\n",
    "correlation_metrics_encoding = get_correlation_metrics(neural_embeddings, predictions)\n",
    "\n",
    "cosine_sim_metrics_encoding = cosine_similarity(neural_embeddings, predictions)\n",
    "\n",
    "mspe_encoding = np.square(neural_embeddings - predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cbc1dd68-64df-4f20-b286-833b59612396",
   "metadata": {
    "id": "lpNuTDxtQM5G"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_correlation': np.float64(0.08921885311122751),\n",
       " 'overall_p_value': np.float32(0.0),\n",
       " 'dimension_correlation': array([-0.05403258, -0.03320565,  0.09859984,  0.05056833,  0.07959674,\n",
       "        -0.04967352, -0.0312699 ,  0.09559993,  0.06677794,  0.07828864,\n",
       "         0.04077907, -0.05981023,  0.05970785,  0.06430414,  0.09231448,\n",
       "        -0.04050403]),\n",
       " 'embedding_correlation': array([ 0.31093988,  0.06866968,  0.06528332, ...,  0.44671828,\n",
       "         0.42354782, -0.60863047]),\n",
       " 'mean_dimension_correlation': np.float64(0.02862756584024372),\n",
       " 'mean_embedding_correlation': np.float64(0.05821150774730866),\n",
       " 'median_dimension_correlation': np.float64(0.05513809018072449),\n",
       " 'median_embedding_correlation': np.float64(0.13116525405348511)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f472fc3a-a319-47e2-8857-5e3cadd8627d",
   "metadata": {
    "id": "xE1VywbKcLM9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(39987930000.0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSPE is not very useful in high dimensions, prefer to use cosine similarity or correlation (which is actually just cosine similarity with 0 mean)\n",
    "mspe_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4dc06e9-580e-4742-bc0a-a95a75b129f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2699872 ,  0.04146124,  0.03587702, ...,  0.43757004,\n",
       "        0.42838106, -0.6119749 ], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are primarily interested in the diagonal distances. ith prediction compared to ith groundtruth.\n",
    "np.diag(cosine_sim_metrics_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414b29f-09b4-4413-8cb6-ec3dfd1d8b54",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616fa3c-166c-4cb9-baff-ade3a8d98d28",
   "metadata": {},
   "source": [
    "Note that decoding is just encoding but with the input and target variables switched. Now neural embeddings -> word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b783d25-8986-4ea5-a98f-78d0402c27f0",
   "metadata": {
    "id": "wtikyMCVcLs6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for electrode 9. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_9.mat.\n",
      "No files found for electrode 59. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_59.mat.\n",
      "No files found for electrode 60. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_60.mat.\n",
      "No files found for electrode 61. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_61.mat.\n",
      "padding mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "decoding_data_config = merge_data_configs(\n",
    "    encoding_experiment_config.encoding_data_config, ecog_config.ecog_data_config\n",
    ")\n",
    "\n",
    "dataset = EncodingDecodingDataset(encoding_data_config)\n",
    "\n",
    "word_embeddings, neural_embeddings = generate_embedding_dataset(\n",
    "    dataset,\n",
    "    model,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_batch_size,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_device,\n",
    ")\n",
    "\n",
    "predictions = run_regression(\n",
    "    neural_embeddings,\n",
    "    word_embeddings,\n",
    "    encoding_experiment_config.encoding_task_config.num_folds,\n",
    ")\n",
    "\n",
    "correlation_metrics_decoding = get_correlation_metrics(word_embeddings, predictions)\n",
    "mspe_decoding = np.square(word_embeddings - predictions).mean()\n",
    "cosine_sim_metrics_decoding = cosine_similarity(word_embeddings, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd389d3b-56ec-4e55-9ff3-07c55139eec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_correlation': np.float64(0.6612710062173627),\n",
       " 'overall_p_value': np.float64(0.0),\n",
       " 'dimension_correlation': array([ 2.04295270e-02, -2.27029405e-02,  8.06532269e-03, -2.12710066e-02,\n",
       "        -6.64142614e-02,  3.19834851e-02, -3.93059692e-02, -4.33604094e-02,\n",
       "        -1.79393752e-02, -2.98978060e-02,  1.20554469e-03, -1.68862287e-02,\n",
       "        -9.39533549e-04, -4.70182909e-03,  7.42343494e-03, -3.55270492e-02,\n",
       "        -3.17225109e-02, -2.73973549e-03, -5.69697863e-02,  4.74830362e-02,\n",
       "        -6.90403091e-02, -9.73817257e-02, -1.17746033e-01, -4.77820765e-02,\n",
       "        -3.18949500e-02, -6.14904572e-02, -9.68584119e-03,  2.23279083e-02,\n",
       "        -1.79587090e-02, -1.14882871e-02, -7.08157595e-02, -1.32721398e-01,\n",
       "         3.87375975e-03, -6.24297426e-02, -2.24568150e-03, -4.42415503e-02,\n",
       "        -4.66377526e-02, -2.17995207e-02, -4.17129422e-02, -3.01430513e-02,\n",
       "        -6.21342661e-02, -2.16423516e-02, -2.67077060e-03, -2.99814471e-02,\n",
       "        -2.49954981e-02, -5.71153934e-02, -8.41401461e-03, -7.13852483e-03,\n",
       "        -1.17676555e-02, -1.59878360e-02, -3.95125934e-02,  1.75224788e-04,\n",
       "        -3.11320006e-02,  1.09125995e-02, -3.17193486e-03, -1.64608149e-02,\n",
       "         5.22885736e-03, -5.34696698e-02,  2.49645487e-02, -3.01744326e-02,\n",
       "        -7.71204003e-02, -3.01603474e-02, -2.42024237e-02, -2.36436478e-02,\n",
       "        -3.58262597e-01, -1.28455478e-01, -3.79524320e-02, -1.12160089e-01,\n",
       "         2.05445100e-02, -4.70468113e-02, -2.55117285e-02, -1.19911606e-02,\n",
       "        -6.80360298e-03, -1.28361940e-02,  9.72750382e-03, -5.96675648e-02,\n",
       "        -7.62485567e-02, -6.78125362e-02, -2.64712859e-02, -1.50232990e-02,\n",
       "        -3.15789201e-02, -6.07490563e-02,  5.40159243e-02, -5.92804316e-02,\n",
       "        -7.68543434e-03, -4.41181124e-02, -2.82947358e-01, -2.44145282e-01,\n",
       "        -4.61196808e-03, -6.83584140e-02, -4.95742786e-02,  4.50835024e-02,\n",
       "         1.30643852e-02, -1.76922424e-02, -3.48831745e-02, -3.38265258e-03,\n",
       "        -9.73543782e-02, -7.05253939e-02,  4.28982605e-02, -2.42641767e-02,\n",
       "         2.50119649e-03, -4.71223325e-02, -5.09992179e-02, -6.63393115e-02,\n",
       "        -6.53559496e-02, -1.11942517e-01, -5.86886881e-02, -2.32632991e-02,\n",
       "        -6.96116576e-03, -3.46824941e-02,  6.39904816e-03, -8.36688023e-03,\n",
       "        -9.24280549e-02, -8.49462921e-03,  6.91630815e-04, -9.57159316e-03,\n",
       "        -6.82479732e-02, -4.21878057e-02, -2.42335622e-02, -9.85506349e-04,\n",
       "        -9.46463484e-03, -9.62352594e-02, -1.01476219e-01,  2.99208207e-02,\n",
       "         2.62375776e-02,  4.81505449e-02, -1.43120585e-02, -2.42168189e-02,\n",
       "         3.82930181e-02,  3.09029294e-02, -7.58791647e-03, -3.19548240e-02,\n",
       "        -2.27897345e-02, -1.25646449e-02,  1.13399380e-02, -9.16243622e-03,\n",
       "        -3.20842985e-02, -1.45613834e-02, -1.49953452e-02, -2.53651201e-02,\n",
       "        -1.88630054e-02, -2.67908535e-02, -5.12566765e-02, -4.97422708e-04,\n",
       "        -1.13143494e-01, -6.68971910e-02,  1.75459239e-03, -6.10339839e-02,\n",
       "        -3.86630240e-02, -6.46730414e-02,  2.79807398e-02,  5.02296045e-03,\n",
       "        -3.90199564e-02, -3.25397431e-02,  1.62580820e-02, -6.70064879e-02,\n",
       "        -1.09279519e-01,  6.92445190e-03, -1.02530599e-02,  8.61753426e-03,\n",
       "        -3.19302395e-02, -3.67840070e-02,  3.74138809e-03, -4.44912920e-02,\n",
       "         7.12258197e-03, -6.24624081e-02, -2.24236393e-02, -7.25675485e-02,\n",
       "        -8.17843337e-02, -1.12469943e-01,  2.85779245e-02,  1.07637427e-03,\n",
       "         6.26273870e-04, -4.94756638e-02, -2.50533196e-02, -6.22808167e-02,\n",
       "        -3.53326104e-02,  2.22761646e-02, -5.99678297e-02, -3.50476557e-03,\n",
       "        -3.15226621e-02, -2.49001547e-02, -1.49256222e-01,  2.19974267e-02,\n",
       "         3.81453114e-02, -2.19941701e-02, -9.54949488e-02,  4.48697184e-03,\n",
       "        -9.35947719e-02, -3.62457741e-02,  2.38809868e-02,  2.18638064e-02,\n",
       "        -4.79238956e-02, -5.21523529e-02,  1.81599181e-02,  2.99050571e-02,\n",
       "        -1.70720437e-02, -1.17657113e-01,  2.73626262e-02, -5.63228607e-02,\n",
       "        -3.19192465e-02, -3.27963439e-02, -1.37462329e-02, -3.93068088e-02,\n",
       "         1.26546486e-02, -5.81267389e-02, -1.05915343e-01,  1.91190983e-02,\n",
       "        -9.64452855e-02,  1.36773299e-02,  1.48509749e-02,  4.36446816e-02,\n",
       "        -2.16714008e-02, -2.85273089e-03, -5.08539435e-02,  1.81730021e-02,\n",
       "        -2.54188326e-02,  1.61721802e-02, -4.11008685e-02, -3.88843979e-02,\n",
       "         2.06961882e-02,  5.70035164e-02, -6.92672024e-02, -4.20393990e-03,\n",
       "        -3.53110117e-02, -1.66899707e-02, -3.16663789e-02, -3.00642684e-04,\n",
       "        -5.90039072e-02, -4.43782743e-02,  1.99085094e-02,  3.40879323e-02,\n",
       "         1.19041173e-02, -2.64400241e-02, -1.10957144e-01, -5.85421604e-02,\n",
       "         1.08254090e-02,  2.67378287e-02,  2.20747104e-02, -3.17781083e-02,\n",
       "         6.95270592e-03, -9.86793742e-02, -2.86698451e-02, -3.35957448e-02,\n",
       "        -4.35476809e-02, -7.09328328e-03, -8.67932341e-03, -7.61912645e-02,\n",
       "        -7.20758712e-03, -7.47607663e-02, -1.64425109e-02, -8.73338033e-02,\n",
       "        -1.31447805e-02, -5.59793730e-03, -2.37127896e-02,  1.05093052e-02,\n",
       "        -1.16709515e-01, -8.95260982e-02, -1.31309260e-02, -2.01830716e-02,\n",
       "        -8.60783116e-02, -2.79722139e-02, -1.58064224e-02,  5.04733411e-03,\n",
       "        -1.54337920e-02, -5.33680971e-02, -2.63329550e-01,  1.78117001e-02,\n",
       "         2.62949271e-03, -5.47343271e-02, -1.27379254e-02,  1.00100274e-02,\n",
       "        -8.48645570e-03,  3.11011137e-02, -2.06784355e-02,  1.12781402e-02,\n",
       "        -5.45869840e-02, -2.79101198e-02, -3.96989579e-02,  8.52234570e-03,\n",
       "        -9.02131855e-03, -2.91657710e-02, -5.44751897e-02, -8.21945336e-02,\n",
       "        -2.77175685e-02, -8.60271030e-02, -2.13478460e-02,  2.29070856e-02,\n",
       "        -4.59461265e-02, -1.01270830e-02, -3.68696849e-02, -7.51884405e-03,\n",
       "         3.94522206e-03, -1.85640186e-02,  1.68400225e-02, -4.75380853e-02,\n",
       "        -1.02435904e-02, -7.28052557e-03, -4.21139108e-02, -1.88957753e-02,\n",
       "        -3.20461991e-02, -2.93552354e-02, -2.41279336e-02,  2.51190506e-02,\n",
       "        -5.12408410e-02, -7.39687378e-03, -6.96831347e-02, -4.42828757e-02,\n",
       "         1.16458342e-02,  1.90991627e-02, -9.38931001e-02, -1.01190008e-01,\n",
       "        -5.63377191e-02,  4.79782711e-04,  3.10151727e-02, -1.38722552e-03,\n",
       "        -6.61530330e-02,  1.60882913e-02, -4.85495461e-02, -1.71345520e-02,\n",
       "        -8.03321330e-03, -1.10853724e-02, -8.27232749e-02, -5.66577676e-02,\n",
       "        -3.67292470e-02, -1.76819270e-02, -2.71245976e-02,  3.86253729e-02,\n",
       "        -4.45960099e-02, -1.40926092e-02, -6.91798462e-03, -1.31968110e-01,\n",
       "         3.44839482e-03, -3.86589473e-02, -4.40875245e-02, -2.34122967e-02,\n",
       "        -7.97415651e-03,  3.74061483e-03, -2.32432109e-02,  2.24336764e-02,\n",
       "         7.39282691e-03,  1.78318145e-02, -1.97500837e-02, -1.73133963e-03,\n",
       "         2.71142895e-02, -4.67961610e-02, -3.88791888e-02, -3.31485700e-02,\n",
       "        -2.76266582e-02, -5.12654179e-02, -6.57867654e-02,  4.49523835e-03,\n",
       "        -1.61095731e-02, -2.04615706e-02, -7.17782308e-02, -1.55128417e-02,\n",
       "        -1.45724581e-02, -5.76288470e-02, -2.28411523e-02, -4.05651615e-02,\n",
       "        -1.00568340e-03, -4.69593343e-02,  1.35406262e-02,  2.50887087e-02,\n",
       "        -2.40816589e-02, -8.82492241e-03, -8.30619687e-02, -3.66057792e-03,\n",
       "        -7.71262127e-02,  3.87771509e-03, -2.56297816e-02, -4.80125170e-02,\n",
       "        -1.35842639e-01, -1.91603574e-01, -8.22297048e-02, -5.33120337e-02,\n",
       "        -6.36001623e-03, -5.75015951e-02, -5.45723774e-02, -1.89185549e-02,\n",
       "        -7.35907326e-02,  1.30607625e-03, -1.73900754e-02,  2.02421966e-02,\n",
       "        -4.28377610e-02,  2.52365886e-02, -1.00947080e-02,  1.92637919e-02,\n",
       "        -2.39410328e-02, -8.58731490e-02,  2.97305444e-02, -1.05958332e-01,\n",
       "        -4.97839595e-02, -1.48170790e-02, -8.19568047e-02, -2.36046326e-02,\n",
       "        -6.06414911e-02, -4.19003278e-02, -1.92130479e-02, -2.30961875e-02,\n",
       "        -4.41393720e-02, -6.92355615e-02, -8.54860143e-03, -3.56507963e-02,\n",
       "        -7.28417673e-02, -1.63552540e-03, -8.36361871e-02,  8.97174283e-03,\n",
       "         1.85423932e-02, -7.22189056e-02, -5.15441020e-02,  2.36619465e-02,\n",
       "        -1.18527721e-01, -1.17694257e-02,  2.59295209e-04, -2.72586966e-02,\n",
       "        -1.44358848e-02,  2.07301650e-02, -2.91909901e-02,  5.29787337e-02,\n",
       "        -1.68890120e-03, -5.74047326e-02, -8.16282866e-02, -7.55697111e-03,\n",
       "         5.18990029e-03,  1.48230538e-02, -5.32279291e-02,  9.96696533e-03,\n",
       "        -1.75252277e-02,  3.70134696e-03, -5.70102221e-03, -4.25417822e-03,\n",
       "        -3.98172236e-02, -4.31629303e-02, -4.35715319e-02,  2.65721131e-02,\n",
       "         6.51534813e-02, -1.18329016e-01,  1.79167145e-02, -1.11795384e-01,\n",
       "         1.81236977e-02, -9.14772351e-05, -2.25107946e-02, -5.43128915e-03,\n",
       "        -3.41676754e-03, -4.96729388e-02, -2.48410551e-02, -7.07998409e-02,\n",
       "         8.39514392e-03, -1.55868333e-02, -9.65249116e-02, -1.36631607e-02,\n",
       "        -1.32972182e-02, -3.25674486e-02, -3.37090435e-03, -3.61349720e-03,\n",
       "         8.22234569e-03, -5.47555343e-02, -4.49266538e-02, -2.05643386e-02,\n",
       "        -1.99163322e-02, -1.72216780e-02, -2.07998133e-02, -1.63881422e-02,\n",
       "        -2.09527012e-01, -1.68944264e-02, -1.07187097e-01,  1.39062287e-02,\n",
       "        -1.08978570e-02, -5.52561337e-02, -1.22332432e-02, -1.08347779e-01,\n",
       "        -1.87489642e-02, -5.64976270e-02, -3.88067419e-02,  7.36996495e-03,\n",
       "         6.87530898e-03,  3.70001382e-03, -2.20396781e-02, -2.20381209e-02,\n",
       "        -3.11458586e-02, -4.61412892e-02, -4.26549814e-02, -5.11985350e-03,\n",
       "        -4.71748019e-02, -7.11839152e-02,  2.05002512e-02, -6.15710051e-02,\n",
       "        -3.89252794e-02,  1.25740463e-02, -1.59301914e-02, -4.12360969e-02,\n",
       "         3.20646389e-02, -3.75658809e-02, -6.76662984e-02, -1.10940406e-01,\n",
       "         3.70232575e-02, -2.07662687e-02, -6.36714939e-02, -3.45404731e-02,\n",
       "        -5.72294173e-02, -7.86095336e-03, -1.69280267e-02, -5.20195695e-02,\n",
       "         2.63738958e-02, -4.11950027e-02,  1.68354630e-02,  2.10459895e-02,\n",
       "        -3.22119511e-02,  2.85308481e-02, -1.11698559e-01, -3.67596268e-02,\n",
       "        -1.58391927e-02, -9.55668407e-03, -2.93689515e-02, -1.80600386e-02,\n",
       "        -2.48474253e-02,  7.28162194e-04, -5.22413270e-02, -9.91127681e-02,\n",
       "        -6.76632710e-02, -9.79966301e-02,  3.38692350e-02, -4.87688534e-02,\n",
       "        -4.85565460e-02, -2.11798465e-02, -6.52388104e-02,  5.45330716e-03,\n",
       "         3.04913010e-02, -7.34138570e-04, -5.68162464e-02,  4.50884463e-04,\n",
       "        -1.72214277e-02, -3.12956621e-02, -7.20131857e-02, -4.75044357e-02,\n",
       "        -2.26756629e-02, -4.03153725e-02, -3.17147278e-02, -1.06759868e-01,\n",
       "        -2.48076707e-02, -6.75452226e-03, -5.70634728e-02, -3.47389625e-02,\n",
       "         4.55720332e-03, -8.21294536e-02, -6.64632347e-02, -7.51147816e-02,\n",
       "        -1.70814535e-02,  9.03689588e-03, -3.65560955e-02,  2.18998748e-02,\n",
       "        -1.68326995e-03, -7.55735139e-02, -4.08564284e-02, -1.63589822e-01,\n",
       "        -1.25802685e-01, -4.16837520e-04, -8.49341270e-02,  2.44801997e-02,\n",
       "        -3.38961024e-02, -3.03118474e-02, -2.65618748e-02, -6.78970908e-03,\n",
       "         3.44718263e-02, -1.58605325e-02, -5.10823805e-02,  3.75193922e-02,\n",
       "         6.29688764e-03, -5.92707889e-02, -1.20739602e-02, -5.29674332e-03,\n",
       "        -3.61738032e-02,  2.72042860e-02, -6.76521050e-02, -1.74659271e-02,\n",
       "        -9.90530083e-02,  5.53645489e-04, -1.50897135e-01,  1.05499934e-02,\n",
       "        -1.50654118e-01,  1.57763193e-02,  5.43096502e-03, -2.49172669e-02,\n",
       "        -8.51633878e-02, -1.16461454e-02, -2.89262735e-02, -1.47367722e-02,\n",
       "        -1.08685165e-02, -6.45209629e-02, -4.29635042e-02, -1.05331312e-02,\n",
       "        -1.54981461e-02, -3.68233140e-02,  3.73469448e-02, -4.27537980e-02,\n",
       "        -4.64226820e-02, -3.39078718e-02, -3.84413135e-02, -1.65891870e-02,\n",
       "        -9.21991332e-02, -3.43626934e-03, -2.34825754e-02,  2.63735071e-02,\n",
       "        -7.46430482e-03,  5.52612901e-04, -3.97745415e-02,  3.82399159e-02,\n",
       "        -5.46900255e-02, -2.78014859e-02, -5.73739586e-02, -4.16018819e-02,\n",
       "        -1.25417061e-01,  4.00010141e-03,  1.84630690e-03, -7.72472751e-03,\n",
       "        -4.83037128e-02, -5.39048651e-02,  6.37910482e-04, -2.30522483e-02,\n",
       "        -6.24554153e-02, -7.97621429e-02, -1.17733836e-02, -2.66287529e-02,\n",
       "         4.97338200e-03,  1.28976889e-02, -4.96234859e-02,  2.65808243e-05,\n",
       "        -4.06849553e-02,  9.61347747e-05, -4.24849774e-02, -8.92021155e-02,\n",
       "        -6.69091727e-02,  1.31846108e-02, -5.01438180e-03,  1.57556353e-02,\n",
       "         2.96009656e-02, -1.55005499e-02,  2.43953360e-02, -4.35955703e-02,\n",
       "         1.21648167e-02,  9.50381659e-03, -8.86661886e-02, -4.13363107e-02,\n",
       "        -3.43945214e-02, -1.62093771e-02, -2.28376439e-02, -1.16394511e-02,\n",
       "        -5.28110351e-02,  2.47447653e-03, -1.27973717e-02,  6.65967337e-03,\n",
       "        -3.00814171e-02, -1.30282647e-01,  3.51665139e-02, -6.66432768e-02,\n",
       "         2.29990361e-02,  1.11209238e-02, -2.18559590e-02, -9.76888576e-03,\n",
       "        -3.83629875e-02, -1.34648761e-02, -1.56161015e-02, -6.91022617e-02,\n",
       "        -5.99589773e-02, -3.53269894e-02, -1.00059266e-01, -9.93796213e-02,\n",
       "        -4.79235844e-02,  5.52465558e-03, -5.84496885e-02, -3.54093746e-02,\n",
       "        -8.24119101e-04, -3.11462324e-02, -5.73072468e-02, -1.59710831e-02,\n",
       "        -9.41096128e-02, -8.91436360e-04, -2.51283934e-02, -5.43701244e-02,\n",
       "        -1.47177214e-01,  7.48018349e-03, -2.51075220e-02,  2.01364934e-02,\n",
       "        -6.27985127e-03, -1.26236136e-02, -3.70555000e-03, -3.68886802e-02,\n",
       "        -6.78441896e-03, -1.65239501e-02, -8.68763365e-02,  4.20836487e-03,\n",
       "        -3.20905215e-02, -2.81375161e-02, -7.03352193e-02,  1.02269881e-02,\n",
       "        -1.55950892e-01,  5.53489581e-02, -5.79156010e-02, -5.48677405e-03,\n",
       "        -3.56308477e-02, -2.89479690e-02, -2.78640723e-02, -2.07214469e-02,\n",
       "         1.98188747e-02, -1.49524107e-02,  1.67788519e-02,  7.74276122e-03,\n",
       "        -3.22945756e-02, -1.25398070e-02, -2.65541003e-02, -1.14486678e-01,\n",
       "        -1.69224472e-02,  6.16356281e-03, -5.62694682e-02, -3.70968767e-03,\n",
       "        -2.44673251e-02, -1.54202262e-03, -7.45409492e-03, -4.40879406e-02,\n",
       "        -2.37389157e-02,  3.29630950e-02, -5.57952607e-02, -5.74834908e-02,\n",
       "         1.64347340e-02,  1.48253530e-02, -9.68797668e-03, -3.93653039e-02,\n",
       "        -1.25516685e-02, -1.98200779e-02, -7.77656940e-02, -5.45916960e-03,\n",
       "        -1.29751178e-02, -2.33095710e-02,  1.06281957e-02, -9.08915287e-03,\n",
       "         2.94199148e-02, -1.03367290e-02, -2.23249100e-03, -8.22022505e-03,\n",
       "        -2.64745801e-02, -2.80592533e-02, -1.21385517e-01, -2.22894189e-03,\n",
       "        -1.80811156e-02, -4.79864978e-02, -4.04596437e-02, -3.99649982e-02,\n",
       "         3.09431816e-02,  2.63814822e-02, -2.28666644e-02, -6.57735641e-02,\n",
       "        -7.11085721e-02, -6.66124629e-02, -5.12530885e-02, -3.05655647e-02,\n",
       "        -2.48123516e-01,  1.36412020e-02, -5.47558056e-02, -5.54447243e-02,\n",
       "        -1.93314474e-02, -5.40824922e-02, -1.13189724e-02, -2.53834352e-02,\n",
       "         1.52772873e-02, -2.59644785e-02, -1.02520921e-01, -5.51955307e-02]),\n",
       " 'embedding_correlation': array([0.5398755 , 0.52175421, 0.44860735, ..., 0.7460759 , 0.61853292,\n",
       "        0.65752089]),\n",
       " 'mean_dimension_correlation': np.float64(-0.029376138392987052),\n",
       " 'mean_embedding_correlation': np.float64(0.6648186218039307),\n",
       " 'median_dimension_correlation': np.float64(-0.023169699240060956),\n",
       " 'median_embedding_correlation': np.float64(0.6855383966069721)}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4fd7e2a-8d47-4d77-85f4-fa54a3d0db1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.666171667972233)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspe_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4dc7be25-751c-4e32-a604-756cf9b1ac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5400652 , 0.52196837, 0.44877155, ..., 0.74616543, 0.61855163,\n",
       "       0.65767508])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(cosine_sim_metrics_decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e8c87-701b-4b2b-888d-06ac095ad50d",
   "metadata": {},
   "source": [
    "### Baselining these tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4bea6-252c-474a-b48b-c3581fccb10f",
   "metadata": {},
   "source": [
    "As a baseline let's run encoding and decoding with an untrained model which can be used to see what effects training has had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78e8229a-2c00-41f1-a470-c00f5e9072c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches 1024\n",
      "num_encoder_patches 768\n",
      "num_decoder_patches 1024\n",
      "patch dimensionality 16\n",
      "encoder embedding dimensionality 16\n",
      "decoder embedding dimensionality 16\n",
      "img_size (8, 8) patch_size (2, 2) frames 128 t_patch_size 4\n",
      "model initialized\n",
      "param counts:\n",
      "22,448 total\n",
      "22,448 trainable\n"
     ]
    }
   ],
   "source": [
    "untrained_model, _, _ = setup_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40603de-22ce-491d-a342-6a78cedfdfaa",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22dcb9d9-e2f2-4402-ba2f-57057e1d5499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for electrode 9. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_9.mat.\n",
      "No files found for electrode 59. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_59.mat.\n",
      "No files found for electrode 60. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_60.mat.\n",
      "No files found for electrode 61. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_61.mat.\n",
      "padding mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "encoding_data_config = merge_data_configs(\n",
    "    encoding_experiment_config.encoding_data_config, ecog_config.ecog_data_config\n",
    ")\n",
    "\n",
    "dataset = EncodingDecodingDataset(encoding_data_config)\n",
    "\n",
    "word_embeddings, neural_embeddings = generate_embedding_dataset(\n",
    "    dataset,\n",
    "    untrained_model,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_batch_size,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_device,\n",
    ")\n",
    "\n",
    "predictions = run_regression(\n",
    "    word_embeddings,\n",
    "    neural_embeddings,\n",
    "    encoding_experiment_config.encoding_task_config.num_folds,\n",
    ")\n",
    "\n",
    "correlation_metrics_enc_baseline = get_correlation_metrics(neural_embeddings, predictions)\n",
    "\n",
    "cosine_sim_metrics_enc_baseline = cosine_similarity(neural_embeddings, predictions)\n",
    "\n",
    "mspe_enc_baseline = np.square(neural_embeddings - predictions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0e27847c-7963-4537-8461-a8e617fbc90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_correlation': np.float64(0.08848809764511045),\n",
       " 'overall_p_value': np.float32(0.0),\n",
       " 'dimension_correlation': array([0.096358  , 0.10162078, 0.06384222, 0.04915722, 0.00860763,\n",
       "        0.05549028, 0.09215233, 0.04644525, 0.07399268, 0.06009063,\n",
       "        0.02300932, 0.05116737, 0.07779752, 0.08051954, 0.07855005,\n",
       "        0.01517785]),\n",
       " 'embedding_correlation': array([ 0.55572575,  0.32568157,  0.32022963, ...,  0.18394756,\n",
       "         0.13500682, -0.65420628]),\n",
       " 'mean_dimension_correlation': np.float64(0.06087366704789632),\n",
       " 'mean_embedding_correlation': np.float64(0.06023617899148209),\n",
       " 'median_dimension_correlation': np.float64(0.061966425145922696),\n",
       " 'median_embedding_correlation': np.float64(0.11543528485344723)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_enc_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9bf6350b-1471-463a-bb9f-70af2734100e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.55125505,  0.31048352,  0.30484495, ...,  0.21237554,\n",
       "        0.1593564 , -0.6508919 ], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(cosine_sim_metrics_enc_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49779df0-95cf-43b3-b74d-c63c3d3a9404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(102005750000.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspe_enc_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "849e63c7-410c-4068-a2cb-ea52817a1fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.5509136)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mspe_enc_baseline - mspe_encoding) / mspe_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa0113-44e6-467e-9ac6-815052597f56",
   "metadata": {},
   "source": [
    "Baseline has larger mse than trained model, but again it's high-dimensional data so take it with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8fdee2fe-58cc-43bb-a5e1-a00fa5b4fe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.058246408)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.diag(cosine_sim_metrics_enc_baseline).mean() - np.diag(cosine_sim_metrics_encoding).mean()) / np.diag(cosine_sim_metrics_encoding).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16257877-f92c-48b7-a87c-665a38e41ed1",
   "metadata": {},
   "source": [
    "Baseline has a slightly higher cosine similarity so not as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b5e85fc-b4f9-4bd1-9c0b-15933e166c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15039058,  0.13482644, -0.03475762, -0.00141111, -0.07098911,\n",
       "        0.1051638 ,  0.12342223, -0.04915468,  0.00721474, -0.01819801,\n",
       "       -0.01776974,  0.1109776 ,  0.01808966,  0.0162154 , -0.01376442,\n",
       "        0.05568187])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_enc_baseline['dimension_correlation'] - correlation_metrics_encoding['dimension_correlation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e742ae8-8a51-4307-9311-6688869253bf",
   "metadata": {},
   "source": [
    "Again, nothing super positive here. Let's improve on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9daf7-9dcb-4eb8-acc9-c68111a1ef45",
   "metadata": {},
   "source": [
    "#### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "201b779d-7b00-4916-9342-0c31c75e4e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for electrode 9. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_9.mat.\n",
      "No files found for electrode 59. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_59.mat.\n",
      "No files found for electrode 60. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_60.mat.\n",
      "No files found for electrode 61. Got 0 files matching ../preprocessed-highgamma/NY*_*_Part*_conversation*_electrode_preprocess_file_61.mat.\n",
      "padding mask: tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "encoding_data_config = merge_data_configs(\n",
    "    encoding_experiment_config.encoding_data_config, ecog_config.ecog_data_config\n",
    ")\n",
    "\n",
    "dataset = EncodingDecodingDataset(encoding_data_config)\n",
    "\n",
    "word_embeddings, neural_embeddings = generate_embedding_dataset(\n",
    "    dataset,\n",
    "    untrained_model,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_batch_size,\n",
    "    encoding_experiment_config.encoding_task_config.embedding_device,\n",
    ")\n",
    "\n",
    "predictions = run_regression(\n",
    "    neural_embeddings,\n",
    "    word_embeddings,\n",
    "    encoding_experiment_config.encoding_task_config.num_folds,\n",
    ")\n",
    "\n",
    "correlation_metrics_dec_baseline = get_correlation_metrics(word_embeddings, predictions)\n",
    "mspe_dec_baseline = np.square(word_embeddings - predictions).mean()\n",
    "cosine_sim_metrics_dec_baseline = cosine_similarity(word_embeddings, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebb4a93d-4673-434c-803a-b7cdfcf0d078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_correlation': np.float64(0.6611910936141596),\n",
       " 'overall_p_value': np.float64(0.0),\n",
       " 'dimension_correlation': array([ 1.73796003e-02, -2.92823232e-02,  1.13802536e-02, -1.95189943e-02,\n",
       "        -6.25930996e-02,  1.91035743e-02, -4.39974425e-02, -3.58155312e-02,\n",
       "        -1.11652904e-02, -4.60108588e-02,  4.20099152e-03, -3.27984040e-02,\n",
       "        -6.65520060e-03, -3.89514673e-03, -2.91521990e-03, -3.08804124e-02,\n",
       "        -3.26501488e-02,  6.34405628e-03, -5.68950694e-02,  4.97219033e-02,\n",
       "        -6.86377649e-02, -9.50502987e-02, -1.19888238e-01, -5.81986792e-02,\n",
       "        -4.65760066e-02, -6.01986083e-02, -1.72498659e-02,  2.07878976e-02,\n",
       "        -2.27047486e-02,  4.40153284e-03, -8.60276245e-02, -1.30315348e-01,\n",
       "         2.52221911e-03, -5.87208319e-02, -1.15735333e-03, -3.07875116e-02,\n",
       "        -4.02347510e-02, -2.26061230e-02, -3.61386331e-02, -3.72706205e-02,\n",
       "        -6.35426351e-02, -1.32225996e-02, -1.09128552e-02, -2.99229244e-02,\n",
       "        -2.41464087e-02, -5.39956912e-02, -5.70894144e-03, -5.17158245e-03,\n",
       "        -2.05941899e-02, -1.36070502e-02, -5.89433149e-02,  8.48624847e-03,\n",
       "        -3.29307241e-02,  1.96308212e-02, -2.05484841e-02, -2.00983932e-02,\n",
       "        -1.92657091e-02, -4.94196467e-02,  2.87263019e-02, -1.77445229e-02,\n",
       "        -8.26699359e-02, -3.19772914e-02, -1.77784216e-02, -2.28805997e-02,\n",
       "        -3.57959055e-01, -1.32154886e-01, -4.06534631e-02, -1.11052682e-01,\n",
       "         1.82027350e-02, -3.99908552e-02, -2.33161667e-02, -4.23386027e-03,\n",
       "        -2.04321137e-02, -4.93208454e-03,  2.18275317e-02, -5.99745565e-02,\n",
       "        -7.78357650e-02, -6.17789836e-02, -2.21676608e-02, -1.33376813e-02,\n",
       "        -3.78500627e-02, -3.27652819e-02,  4.21686477e-02, -6.18912341e-02,\n",
       "        -8.10062285e-03, -5.55660457e-02, -2.86321743e-01, -2.38215600e-01,\n",
       "         3.57540184e-03, -7.05765039e-02, -5.46141371e-02,  3.78133221e-02,\n",
       "         7.28443214e-03, -2.35175603e-02, -3.57485185e-02, -9.73992068e-03,\n",
       "        -1.06782182e-01, -8.25310156e-02,  3.56447952e-02, -2.37931726e-02,\n",
       "         1.20263962e-02, -4.23728099e-02, -4.21731533e-02, -5.42664569e-02,\n",
       "        -6.85779246e-02, -1.20113398e-01, -5.73941095e-02, -2.67425418e-02,\n",
       "        -2.76046003e-03, -4.07634110e-02,  8.31330880e-03, -1.69464792e-02,\n",
       "        -8.18363998e-02, -6.64678278e-03, -1.07613705e-02, -8.50555580e-03,\n",
       "        -5.79813227e-02, -3.89311899e-02, -3.20599198e-02,  2.77945382e-04,\n",
       "        -1.96529922e-02, -8.66945611e-02, -9.87072419e-02,  3.26001988e-02,\n",
       "         1.78754997e-02,  5.21832732e-02, -1.42826290e-02, -2.98778186e-02,\n",
       "         3.41950236e-02,  2.97345134e-02, -4.84986376e-03, -1.91722102e-02,\n",
       "        -2.55351635e-02, -1.12125864e-02,  1.01691747e-02, -2.09829387e-02,\n",
       "        -3.34936025e-02, -1.66995773e-02, -1.49137274e-02, -2.56259728e-02,\n",
       "        -1.93372786e-02, -2.93671477e-02, -3.55487339e-02,  1.90325894e-03,\n",
       "        -1.11262622e-01, -6.69085257e-02, -1.36617899e-02, -5.78470152e-02,\n",
       "        -3.62824553e-02, -6.01277168e-02,  3.23885689e-02,  9.83380361e-03,\n",
       "        -3.02908687e-02, -3.49918667e-02,  1.03940346e-02, -6.66136035e-02,\n",
       "        -1.13090913e-01,  6.83370780e-03,  3.03017835e-04,  1.09006000e-02,\n",
       "        -3.89883659e-02, -3.99402466e-02,  2.73850125e-03, -4.41447634e-02,\n",
       "        -9.26402187e-03, -5.74551942e-02, -2.30871122e-02, -8.27527418e-02,\n",
       "        -5.99266754e-02, -1.09469881e-01,  2.04078751e-02,  1.69022334e-03,\n",
       "         1.13318165e-02, -4.40717645e-02, -3.95506637e-03, -6.49023647e-02,\n",
       "        -3.79274618e-02,  2.04827184e-02, -5.75278362e-02, -3.34036758e-02,\n",
       "        -2.99552867e-02, -2.92851960e-02, -1.60575960e-01,  1.76480826e-02,\n",
       "         4.32230488e-02, -1.54810095e-02, -9.74550912e-02,  3.38697185e-03,\n",
       "        -9.06577231e-02, -3.97080259e-02,  7.91852880e-03,  1.76381594e-02,\n",
       "        -3.97088122e-02, -4.39221873e-02,  1.02621542e-02,  2.49163688e-02,\n",
       "        -1.20039570e-02, -1.29158701e-01,  3.04183600e-02, -5.35807324e-02,\n",
       "        -3.35970414e-02, -3.82242455e-02, -2.74548553e-02, -3.12916757e-02,\n",
       "         1.74491287e-02, -5.80109911e-02, -1.06556590e-01,  1.99212518e-02,\n",
       "        -9.94209258e-02,  1.30158880e-02,  1.48386355e-02,  4.64695756e-02,\n",
       "        -2.00036897e-02, -2.59488796e-02, -5.21755466e-02,  1.11159468e-02,\n",
       "        -2.44372997e-02,  1.37857262e-02, -3.44585263e-02, -4.38287437e-02,\n",
       "         1.61761391e-02,  5.85481512e-02, -6.85479650e-02, -7.16366470e-03,\n",
       "        -3.66419500e-02, -1.52064512e-02, -2.92159548e-02, -4.52578603e-03,\n",
       "        -5.86953693e-02, -4.42129482e-02,  1.86451589e-02,  3.99147661e-02,\n",
       "         1.60801826e-02, -2.70116342e-02, -1.19585626e-01, -6.34655634e-02,\n",
       "         1.06163916e-02,  2.64451858e-02,  2.42221547e-02, -3.49374086e-02,\n",
       "         8.03649198e-03, -1.00002735e-01, -2.98711101e-02, -3.13660067e-02,\n",
       "        -2.11772147e-02, -3.68307293e-03, -6.88052525e-03, -8.61432470e-02,\n",
       "        -1.17274149e-02, -7.96339422e-02, -2.00106743e-02, -8.52251421e-02,\n",
       "         3.33860440e-03, -7.39381572e-03, -1.91809046e-02,  9.52699258e-03,\n",
       "        -1.13706026e-01, -9.11499494e-02, -1.70646084e-02, -1.92457257e-02,\n",
       "        -7.91511721e-02, -2.68045477e-02, -1.29855587e-02, -5.03251067e-04,\n",
       "        -2.61643342e-02, -5.63159494e-02, -2.61842870e-01,  2.44322729e-02,\n",
       "         1.06028625e-02, -4.91638738e-02, -7.02374508e-03,  9.96307129e-03,\n",
       "         3.45009518e-03,  3.29858382e-02, -2.60939428e-02,  1.93958625e-02,\n",
       "        -5.39085730e-02, -3.68553830e-02, -3.77904631e-02,  7.77012825e-03,\n",
       "        -8.49607241e-03, -3.97533153e-02, -5.46196743e-02, -7.81581752e-02,\n",
       "        -2.36847706e-02, -7.86048283e-02, -3.03883339e-02,  3.52776821e-02,\n",
       "        -5.08961059e-02, -1.86059520e-02, -4.68079016e-02, -6.49854298e-03,\n",
       "        -6.50797598e-03, -2.46735647e-02,  1.38823662e-02, -4.33756254e-02,\n",
       "        -1.09897285e-02, -3.03734002e-03, -5.27951992e-02, -5.09078060e-03,\n",
       "        -3.56067474e-02, -2.27498978e-02, -2.07453662e-02,  2.87752539e-02,\n",
       "        -5.46100924e-02, -1.05803363e-02, -7.28588317e-02, -4.59326464e-02,\n",
       "         2.50902822e-02,  1.48721590e-02, -9.04539997e-02, -1.02040085e-01,\n",
       "        -6.99898016e-02, -3.82219175e-04,  2.78349833e-02,  7.15580137e-03,\n",
       "        -6.21448571e-02,  1.25752710e-02, -5.19544474e-02, -1.45268818e-02,\n",
       "         2.56008430e-03, -9.99871397e-03, -8.11349783e-02, -6.15597903e-02,\n",
       "        -4.34532502e-02, -1.63808094e-02, -3.10234072e-02,  3.59889651e-02,\n",
       "        -4.00520834e-02, -3.21345075e-02, -1.78670341e-03, -1.31178791e-01,\n",
       "         1.75731341e-02, -3.77673369e-02, -4.76998567e-02, -4.48280654e-02,\n",
       "        -2.08497549e-02, -8.49684674e-04, -2.60057310e-02,  1.87346205e-02,\n",
       "         1.48620863e-02,  2.68192179e-02, -1.37757351e-02, -1.24281390e-03,\n",
       "         2.01805773e-02, -6.12429246e-02, -2.70644483e-02, -4.04672416e-02,\n",
       "        -3.57614505e-02, -5.13342646e-02, -7.20239253e-02,  2.08938820e-03,\n",
       "        -1.38821869e-02, -1.21037120e-02, -6.52598608e-02, -1.70909555e-02,\n",
       "        -1.15536648e-02, -5.24817496e-02, -1.79192483e-02, -3.69107281e-02,\n",
       "        -7.46116848e-03, -4.27197667e-02,  1.18391113e-02,  2.74530072e-02,\n",
       "        -3.32648019e-02, -4.75452491e-03, -5.87120934e-02, -5.21518165e-03,\n",
       "        -7.26602058e-02,  1.21383744e-02, -1.98861136e-02, -5.88979613e-02,\n",
       "        -1.28627547e-01, -1.81533992e-01, -7.44590351e-02, -4.55716952e-02,\n",
       "        -3.56377369e-03, -5.66299126e-02, -5.54793651e-02, -2.94922255e-02,\n",
       "        -6.93297341e-02,  4.69845722e-03, -1.95593132e-02,  2.00835032e-02,\n",
       "        -4.32184708e-02,  2.24766246e-02, -3.02442798e-02,  4.68870554e-03,\n",
       "        -2.91327369e-02, -9.44837595e-02,  7.46248459e-03, -9.47777438e-02,\n",
       "        -5.18221273e-02, -1.72642587e-02, -1.01452330e-01, -1.81736076e-02,\n",
       "        -6.06270617e-02, -3.38193804e-02, -2.38391976e-02, -2.03983276e-02,\n",
       "        -3.72941886e-02, -7.72802513e-02, -1.38000190e-02, -3.96591207e-02,\n",
       "        -6.36240587e-02, -5.60897759e-03, -8.27984106e-02,  8.00407150e-03,\n",
       "         6.26799927e-03, -5.41004251e-02, -4.98627501e-02,  2.09115988e-02,\n",
       "        -1.15949424e-01, -1.08789884e-02,  1.50050957e-02, -2.40921869e-02,\n",
       "        -1.35928538e-02,  1.99490942e-02, -1.96208423e-02,  5.23217405e-02,\n",
       "         3.20155937e-03, -4.95852079e-02, -8.49870047e-02, -1.06749532e-02,\n",
       "         5.89344986e-03,  7.27100215e-03, -4.98564763e-02,  1.10198685e-02,\n",
       "        -2.10078455e-02,  3.71214829e-03, -9.26416655e-03, -1.00613838e-02,\n",
       "        -3.56756051e-02, -4.57878165e-02, -2.81372113e-02,  2.63423545e-02,\n",
       "         6.17720835e-02, -1.22768518e-01,  1.88525473e-02, -1.04003933e-01,\n",
       "        -1.35192360e-03,  3.54533649e-03, -2.24815321e-02,  4.88609624e-03,\n",
       "         5.32087478e-04, -5.17308945e-02, -2.38444322e-02, -5.90112483e-02,\n",
       "         1.56966917e-02, -2.33495912e-02, -8.37224665e-02, -1.57858209e-02,\n",
       "        -1.58488578e-02, -2.53807480e-02,  1.95095096e-03, -1.24644620e-03,\n",
       "         8.64911599e-03, -5.60674187e-02, -4.46336130e-02, -1.40604567e-02,\n",
       "        -1.40275632e-02, -1.43735147e-02, -2.46610255e-02, -1.86205832e-02,\n",
       "        -2.08656981e-01, -1.87623304e-02, -1.06015563e-01, -2.57175576e-03,\n",
       "        -6.62853708e-03, -5.63573656e-02, -2.08329187e-02, -1.08920801e-01,\n",
       "        -3.10201529e-02, -5.59436324e-02, -3.63083817e-02,  7.31173420e-03,\n",
       "         7.72811915e-03,  3.16002230e-03, -7.43893462e-03, -3.14439915e-02,\n",
       "        -2.88741662e-02, -3.17954902e-02, -3.68843568e-02, -7.75322808e-03,\n",
       "        -4.80194749e-02, -7.17953851e-02,  3.24823468e-02, -6.30177573e-02,\n",
       "        -3.54189676e-02,  2.50638309e-03, -1.57624900e-02, -3.29564396e-02,\n",
       "         3.03618128e-02, -3.39312693e-02, -6.15704820e-02, -1.10356051e-01,\n",
       "         3.93556301e-02, -2.13280721e-02, -7.29764713e-02, -2.99031928e-02,\n",
       "        -5.83880746e-02, -6.83149778e-03, -1.71214417e-02, -5.48917641e-02,\n",
       "         2.56644004e-02, -3.94541600e-02,  1.15420336e-02,  2.93379019e-02,\n",
       "        -3.36493078e-02,  3.15459286e-02, -1.09957214e-01, -3.72311375e-02,\n",
       "         1.21373839e-02, -2.43010579e-02, -3.31841321e-02, -2.35401943e-02,\n",
       "        -2.84814361e-02,  3.51264234e-03, -4.41943684e-02, -9.91192462e-02,\n",
       "        -6.68911738e-02, -9.96984448e-02,  3.96567859e-02, -4.51913691e-02,\n",
       "        -4.98748876e-02, -2.64715169e-02, -6.89217135e-02, -8.18320193e-03,\n",
       "         2.65479614e-02, -5.53555802e-03, -6.07217741e-02, -6.55150903e-03,\n",
       "        -1.22831571e-03, -3.42549295e-02, -7.76464864e-02, -4.79000796e-02,\n",
       "        -2.10377477e-02, -4.39224627e-02, -2.06863970e-02, -1.04833386e-01,\n",
       "        -2.29533135e-02, -1.39122359e-02, -6.05698012e-02, -3.29488054e-02,\n",
       "         9.60714426e-03, -6.99448106e-02, -7.13859060e-02, -7.66368755e-02,\n",
       "        -1.11959123e-02,  6.61184624e-03, -4.07714946e-02,  2.10318716e-02,\n",
       "         9.87194347e-03, -7.62336429e-02, -4.30009070e-02, -1.64146738e-01,\n",
       "        -1.15581246e-01,  2.99737135e-02, -7.89781105e-02,  2.65053720e-02,\n",
       "        -3.69096955e-02, -4.34936761e-02, -2.68167483e-02, -1.41113021e-02,\n",
       "         2.69067637e-02, -1.44491287e-02, -4.53481288e-02,  1.91188065e-02,\n",
       "         4.70641419e-03, -5.67597081e-02, -9.26246259e-03,  5.17241835e-03,\n",
       "        -3.69307672e-02,  3.04484331e-02, -8.07232775e-02, -1.42102233e-02,\n",
       "        -8.58260578e-02, -3.69039301e-03, -1.54407994e-01,  5.57544879e-03,\n",
       "        -1.52828366e-01,  1.22096833e-02,  4.20996068e-03, -2.92210723e-02,\n",
       "        -8.53982834e-02, -2.01658289e-02, -2.98173178e-02, -1.48334673e-03,\n",
       "        -1.35848101e-02, -5.58632602e-02, -4.19934613e-02, -1.39551283e-02,\n",
       "        -1.71880543e-02, -5.07514439e-02,  3.63839088e-02, -2.97611938e-02,\n",
       "        -5.29641393e-02, -5.01271939e-02, -2.76876754e-02, -1.80083052e-02,\n",
       "        -9.40770883e-02, -8.53771804e-03, -3.73496694e-02,  2.89835463e-02,\n",
       "        -8.77734681e-03,  1.29724409e-03, -4.07852546e-02,  3.56657542e-02,\n",
       "        -5.15847024e-02, -3.29670582e-02, -5.64030331e-02, -2.51854933e-02,\n",
       "        -1.18119848e-01, -5.51014611e-04,  1.71462262e-02, -7.29983840e-03,\n",
       "        -4.36650191e-02, -5.62563345e-02, -7.52286185e-03, -2.93562765e-02,\n",
       "        -4.05837400e-02, -6.96673125e-02, -1.21936004e-02, -2.70871092e-02,\n",
       "         6.69943269e-03,  6.97128779e-03, -5.04871385e-02, -1.10129083e-02,\n",
       "        -3.99251655e-02,  6.31294088e-04, -4.25353988e-02, -9.44429308e-02,\n",
       "        -6.23079775e-02,  1.29575384e-02, -5.68249958e-03,  1.46756231e-02,\n",
       "         3.04954592e-02, -1.73583214e-02,  2.31394762e-02, -4.05937918e-02,\n",
       "         8.54338326e-03,  6.33088116e-03, -7.88563555e-02, -2.14028287e-02,\n",
       "        -1.78269084e-02,  1.15676877e-02, -6.59534528e-03, -1.90132149e-02,\n",
       "        -5.08674199e-02,  6.24307568e-03, -1.32041912e-02,  2.17309748e-02,\n",
       "        -3.77719703e-02, -1.30530751e-01,  3.87197731e-02, -6.29867637e-02,\n",
       "         2.44796999e-02,  5.35850608e-03, -3.46415393e-02,  2.00057126e-03,\n",
       "        -3.65500211e-02, -2.73306342e-02, -1.42654717e-02, -5.43034147e-02,\n",
       "        -5.48906722e-02, -3.85661312e-02, -1.03130157e-01, -1.09495795e-01,\n",
       "        -5.27852403e-02,  6.78542517e-03, -5.99097076e-02, -2.79822606e-02,\n",
       "        -1.52722133e-03, -3.60142184e-02, -5.58273253e-02, -2.39520161e-02,\n",
       "        -9.14882242e-02,  7.50082265e-04, -8.85344890e-03, -4.53669570e-02,\n",
       "        -1.52304146e-01,  1.03135952e-02, -2.92683898e-02,  2.09193269e-02,\n",
       "        -5.90712202e-03, -8.56192705e-03, -2.98586243e-03, -3.10598628e-02,\n",
       "        -1.04618383e-03, -5.01657097e-03, -8.35045524e-02, -4.76014985e-03,\n",
       "        -3.29004612e-02, -3.79680386e-02, -7.26312745e-02,  1.25991171e-02,\n",
       "        -1.59272021e-01,  5.50576926e-02, -5.12846906e-02, -1.24013533e-02,\n",
       "        -2.56431181e-02, -2.58177495e-02, -2.09880559e-02, -2.19786916e-02,\n",
       "         2.10436387e-02, -1.88657641e-02,  4.69694856e-03,  9.26533687e-03,\n",
       "        -3.27204757e-02, -1.91104433e-02, -2.63912873e-02, -1.10158911e-01,\n",
       "        -2.08537845e-02,  6.92894113e-03, -5.68177151e-02,  6.72549289e-03,\n",
       "        -2.36766066e-02, -4.48091986e-03, -1.35795530e-02, -2.19217923e-02,\n",
       "        -2.49106811e-02,  2.99067046e-02, -5.45587006e-02, -5.40469170e-02,\n",
       "         1.86403190e-02,  1.34450035e-02, -1.15820079e-02, -3.97959402e-02,\n",
       "        -1.14864085e-02, -2.20299147e-02, -7.80811827e-02, -1.96336342e-03,\n",
       "        -1.04882563e-02, -2.52309078e-02,  1.44602785e-02, -1.07901503e-02,\n",
       "         2.84631789e-02, -7.07815137e-03, -7.92355895e-03, -3.93859460e-03,\n",
       "        -2.46140025e-02, -2.79072598e-02, -1.18121254e-01,  5.30753715e-03,\n",
       "         3.11566509e-03, -5.25434161e-02, -4.47944378e-02, -3.23035353e-02,\n",
       "         1.38984073e-02,  2.54516350e-02, -1.81649802e-02, -7.53545095e-02,\n",
       "        -7.01485260e-02, -7.72179655e-02, -4.12385628e-02, -3.38926696e-02,\n",
       "        -2.42284619e-01,  9.92155426e-03, -5.85779328e-02, -4.75421682e-02,\n",
       "        -2.94729782e-02, -6.03309314e-02, -1.25067024e-02, -2.72041478e-02,\n",
       "         7.55586618e-03, -2.99406423e-02, -9.62773261e-02, -5.71781626e-02]),\n",
       " 'embedding_correlation': array([0.54025916, 0.52329311, 0.45342982, ..., 0.74691556, 0.61945548,\n",
       "        0.66138294]),\n",
       " 'mean_dimension_correlation': np.float64(-0.02930743178417385),\n",
       " 'mean_embedding_correlation': np.float64(0.6647049650775271),\n",
       " 'median_dimension_correlation': np.float64(-0.02381618509279944),\n",
       " 'median_embedding_correlation': np.float64(0.6850769472844678)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_dec_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3f60752-0f9e-4e2f-8a76-6ad706b54ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54044842, 0.5235    , 0.45358908, ..., 0.74700366, 0.61947496,\n",
       "       0.66153047])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(cosine_sim_metrics_dec_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1bf8e85-8290-4965-bf2d-feb3401209ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.667788622834928)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspe_dec_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d778ee92-4120-4ecb-aa98-585a9c1b7762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00021092077411340737)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mspe_dec_baseline - mspe_decoding) / mspe_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "35dad6b9-549d-4cf5-b706-3678a7670351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-0.00017117030505392212)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.diag(cosine_sim_metrics_dec_baseline).mean() - np.diag(cosine_sim_metrics_decoding).mean()) / np.diag(cosine_sim_metrics_decoding).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be58704d-9c40-4420-8815-aa035687f57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04992662e-03, -6.57938278e-03,  3.31493094e-03,  1.75201237e-03,\n",
       "        3.82116186e-03, -1.28799109e-02, -4.69147330e-03,  7.54487824e-03,\n",
       "        6.77408481e-03, -1.61130528e-02,  2.99544683e-03, -1.59121753e-02,\n",
       "       -5.71566705e-03,  8.06682356e-04, -1.03386548e-02,  4.64663675e-03,\n",
       "       -9.27637883e-04,  9.08379177e-03,  7.47168838e-05,  2.23886704e-03,\n",
       "        4.02544251e-04,  2.33142703e-03, -2.14220457e-03, -1.04166027e-02,\n",
       "       -1.46810566e-02,  1.29184894e-03, -7.56402476e-03, -1.54001066e-03,\n",
       "       -4.74603952e-03,  1.58898199e-02, -1.52118650e-02,  2.40604999e-03,\n",
       "       -1.35154064e-03,  3.70891068e-03,  1.08832817e-03,  1.34540387e-02,\n",
       "        6.40300166e-03, -8.06602248e-04,  5.57430911e-03, -7.12756921e-03,\n",
       "       -1.40836905e-03,  8.41975199e-03, -8.24208460e-03,  5.85227163e-05,\n",
       "        8.49089432e-04,  3.11970224e-03,  2.70507317e-03,  1.96694238e-03,\n",
       "       -8.82653436e-03,  2.38078580e-03, -1.94307214e-02,  8.31102368e-03,\n",
       "       -1.79872351e-03,  8.71822163e-03, -1.73765492e-02, -3.63757832e-03,\n",
       "       -2.44945664e-02,  4.05002311e-03,  3.76175319e-03,  1.24299097e-02,\n",
       "       -5.54953564e-03, -1.81694396e-03,  6.42400214e-03,  7.63048161e-04,\n",
       "        3.03542121e-04, -3.69940767e-03, -2.70103111e-03,  1.10740659e-03,\n",
       "       -2.34177501e-03,  7.05595605e-03,  2.19556176e-03,  7.75730033e-03,\n",
       "       -1.36285107e-02,  7.90410950e-03,  1.21000278e-02, -3.06991710e-04,\n",
       "       -1.58720835e-03,  6.03355255e-03,  4.30362513e-03,  1.68561775e-03,\n",
       "       -6.27114256e-03,  2.79837744e-02, -1.18472766e-02, -2.61080245e-03,\n",
       "       -4.15188509e-04, -1.14479332e-02, -3.37438468e-03,  5.92968220e-03,\n",
       "        8.18736991e-03, -2.21808987e-03, -5.03985842e-03, -7.27018037e-03,\n",
       "       -5.77995306e-03, -5.82531781e-03, -8.65343934e-04, -6.35726810e-03,\n",
       "       -9.42780380e-03, -1.20056217e-02, -7.25346534e-03,  4.71004071e-04,\n",
       "        9.52519967e-03,  4.74952265e-03,  8.82606468e-03,  1.20728546e-02,\n",
       "       -3.22197501e-03, -8.17088080e-03,  1.29457863e-03, -3.47924273e-03,\n",
       "        4.20070573e-03, -6.08091689e-03,  1.91426064e-03, -8.57959900e-03,\n",
       "        1.05916551e-02,  1.84784643e-03, -1.14530013e-02,  1.06603736e-03,\n",
       "        1.02666505e-02,  3.25661583e-03, -7.82635757e-03,  1.26345173e-03,\n",
       "       -1.01883573e-02,  9.54069833e-03,  2.76897671e-03,  2.67937809e-03,\n",
       "       -8.36207793e-03,  4.03272822e-03,  2.94295570e-05, -5.66099971e-03,\n",
       "       -4.09799445e-03, -1.16841603e-03,  2.73805271e-03,  1.27826139e-02,\n",
       "       -2.74542900e-03,  1.35205859e-03, -1.17076326e-03, -1.18205025e-02,\n",
       "       -1.40930399e-03, -2.13819385e-03,  8.16178089e-05, -2.60852710e-04,\n",
       "       -4.74273153e-04, -2.57629419e-03,  1.57079426e-02,  2.40068165e-03,\n",
       "        1.88087249e-03, -1.13346417e-05, -1.54163823e-02,  3.18696866e-03,\n",
       "        2.38056862e-03,  4.54532461e-03,  4.40782911e-03,  4.81084315e-03,\n",
       "        8.72908773e-03, -2.45212360e-03, -5.86404734e-03,  3.92884396e-04,\n",
       "       -3.81139428e-03, -9.07440961e-05,  1.05560778e-02,  2.28306573e-03,\n",
       "       -7.05812648e-03, -3.15623959e-03, -1.00288684e-03,  3.46528655e-04,\n",
       "       -1.63866038e-02,  5.00721382e-03, -6.63472866e-04, -1.01851933e-02,\n",
       "        2.18576583e-02,  3.00006199e-03, -8.17004938e-03,  6.13849070e-04,\n",
       "        1.07055426e-02,  5.40389926e-03,  2.10982532e-02, -2.62154808e-03,\n",
       "       -2.59485137e-03, -1.79344621e-03,  2.43999354e-03, -2.98989102e-02,\n",
       "        1.56737542e-03, -4.38504139e-03, -1.13197381e-02, -4.34934417e-03,\n",
       "        5.07773744e-03,  6.51316053e-03, -1.96014237e-03, -1.09999999e-03,\n",
       "        2.93704876e-03, -3.46225190e-03, -1.59624581e-02, -4.22564699e-03,\n",
       "        8.21508339e-03,  8.23016557e-03, -7.89776387e-03, -4.98868825e-03,\n",
       "        5.06808670e-03, -1.15015884e-02,  3.05573380e-03,  2.74212826e-03,\n",
       "       -1.67779491e-03, -5.42790152e-03, -1.37086224e-02,  8.01513309e-03,\n",
       "        4.79448012e-03,  1.15747798e-04, -6.41246908e-04,  8.02153533e-04,\n",
       "       -2.97564026e-03, -6.61441948e-04, -1.23393942e-05,  2.82489398e-03,\n",
       "        1.66771112e-03, -2.30961487e-02, -1.32160314e-03, -7.05705527e-03,\n",
       "        9.81532938e-04, -2.38645406e-03,  6.64234219e-03, -4.94434579e-03,\n",
       "       -4.52004913e-03,  1.54463481e-03,  7.19237396e-04, -2.95972480e-03,\n",
       "       -1.33093828e-03,  1.48351958e-03,  2.45042411e-03, -4.22514335e-03,\n",
       "        3.08537948e-04,  1.65326118e-04, -1.26335051e-03,  5.82683382e-03,\n",
       "        4.17606533e-03, -5.71610070e-04, -8.62848181e-03, -4.92340300e-03,\n",
       "       -2.09017380e-04, -2.92642990e-04,  2.14744427e-03, -3.15930036e-03,\n",
       "        1.08378606e-03, -1.32336104e-03, -1.20126500e-03,  2.22973809e-03,\n",
       "        2.23704662e-02,  3.41021034e-03,  1.79879816e-03, -9.95198256e-03,\n",
       "       -4.51982778e-03, -4.87317586e-03, -3.56816336e-03,  2.10866114e-03,\n",
       "        1.64833849e-02, -1.79587843e-03,  4.53188499e-03, -9.82312649e-04,\n",
       "        3.00348862e-03, -1.62385115e-03, -3.93368240e-03,  9.37345936e-04,\n",
       "        6.92713948e-03,  1.16766619e-03,  2.82086363e-03, -5.55058518e-03,\n",
       "       -1.07305422e-02, -2.94785222e-03,  1.48668033e-03,  6.62057280e-03,\n",
       "        7.97336977e-03,  5.57045331e-03,  5.71418029e-03, -4.69561570e-05,\n",
       "        1.19365509e-02,  1.88472449e-03, -5.41550727e-03,  8.11772224e-03,\n",
       "        6.78411033e-04, -8.94526321e-03,  1.90849476e-03, -7.52217449e-04,\n",
       "        5.25246136e-04, -1.05875443e-02, -1.44484616e-04,  4.03635844e-03,\n",
       "        4.03279794e-03,  7.42227465e-03, -9.04048797e-03,  1.23705966e-02,\n",
       "       -4.94997944e-03, -8.47886898e-03, -9.93821665e-03,  1.02030107e-03,\n",
       "       -1.04531980e-02, -6.10954614e-03, -2.95765631e-03,  4.16245987e-03,\n",
       "       -7.46138135e-04,  4.24318555e-03, -1.06812884e-02,  1.38049947e-02,\n",
       "       -3.56054832e-03,  6.60533760e-03,  3.38256743e-03,  3.65620334e-03,\n",
       "       -3.36925136e-03, -3.18346252e-03, -3.17569699e-03, -1.64977074e-03,\n",
       "        1.34444480e-02, -4.22700367e-03,  3.43910040e-03, -8.50076678e-04,\n",
       "       -1.36520825e-02, -8.62001886e-04, -3.18018942e-03,  8.54302689e-03,\n",
       "        4.00817588e-03, -3.51302026e-03, -3.40490129e-03,  2.60767026e-03,\n",
       "        1.05932976e-02,  1.08665844e-03,  1.58829661e-03, -4.90202279e-03,\n",
       "       -6.72400322e-03,  1.30111759e-03, -3.89880960e-03, -2.63640784e-03,\n",
       "        4.54392653e-03, -1.80418982e-02,  5.13128121e-03,  7.89318725e-04,\n",
       "        1.41247393e-02,  8.91610387e-04, -3.61233218e-03, -2.14157686e-02,\n",
       "       -1.28755983e-02, -4.59029950e-03, -2.76252004e-03, -3.69905594e-03,\n",
       "        7.46925936e-03,  8.98740337e-03,  5.97434853e-03,  4.88525734e-04,\n",
       "       -6.93371218e-03, -1.44467636e-02,  1.18147404e-02, -7.31867161e-03,\n",
       "       -8.13479232e-03, -6.88467464e-05, -6.23715992e-03, -2.40585015e-03,\n",
       "        2.22738613e-03,  8.35785866e-03,  6.51837005e-03, -1.57811387e-03,\n",
       "        3.01879326e-03,  5.14709746e-03,  4.92190402e-03,  3.65443338e-03,\n",
       "       -6.45548508e-03,  4.23956759e-03, -1.70151492e-03,  2.36429851e-03,\n",
       "       -9.18314306e-03,  4.07039750e-03,  2.43498753e-02, -1.55460373e-03,\n",
       "        4.46600697e-03,  8.26065934e-03,  5.74366802e-03, -1.08854443e-02,\n",
       "        7.21509209e-03,  1.00695822e-02,  7.77066969e-03,  7.74033856e-03,\n",
       "        2.79624253e-03,  8.71682477e-04, -9.06987748e-04, -1.05736706e-02,\n",
       "        4.26099852e-03,  3.39238097e-03, -2.16923774e-03, -1.58693353e-04,\n",
       "       -3.80709826e-04, -2.75996392e-03, -2.01495718e-02, -1.45750864e-02,\n",
       "       -5.19170408e-03, -8.61061046e-03, -2.22680598e-02,  1.11805879e-02,\n",
       "       -2.03816786e-03, -2.44717973e-03, -1.94955255e-02,  5.43102496e-03,\n",
       "        1.44294566e-05,  8.08094739e-03, -4.62614972e-03,  2.69785994e-03,\n",
       "        6.84518334e-03, -8.04468977e-03, -5.25141754e-03, -4.00832442e-03,\n",
       "        9.21770864e-03, -3.97345219e-03,  8.37776494e-04, -9.67671333e-04,\n",
       "       -1.22743939e-02,  1.81184805e-02,  1.68135192e-03, -2.75034773e-03,\n",
       "        2.57829611e-03,  8.90437266e-04,  1.47458005e-02,  3.16650970e-03,\n",
       "        8.43030948e-04, -7.81070809e-04,  9.57014781e-03, -6.56993228e-04,\n",
       "        4.89046057e-03,  7.81952468e-03, -3.35871816e-03, -3.11798213e-03,\n",
       "        7.03549574e-04, -7.55205163e-03,  3.37145277e-03,  1.05290320e-03,\n",
       "       -3.48261786e-03,  1.08013359e-05, -3.56314434e-03, -5.80720553e-03,\n",
       "        4.14161857e-03, -2.62488624e-03,  1.54343206e-02, -2.29758586e-04,\n",
       "       -3.38139778e-03, -4.43950151e-03,  9.35832730e-04,  7.79145135e-03,\n",
       "       -1.94756213e-02,  3.63681373e-03,  2.92624870e-05,  1.03173854e-02,\n",
       "        3.94885502e-03, -2.05795574e-03,  9.96622834e-04,  1.17885926e-02,\n",
       "        7.30154783e-03, -7.76275791e-03,  1.28024451e-02, -2.12266017e-03,\n",
       "       -2.55163955e-03,  7.18670063e-03,  5.32185531e-03,  2.36705100e-03,\n",
       "        4.26770304e-04, -1.31188441e-03,  2.93040782e-04,  6.50388187e-03,\n",
       "        5.88876900e-03,  2.84816325e-03, -3.86121217e-03, -2.23244106e-03,\n",
       "        8.70031205e-04, -1.86790400e-03,  1.17153427e-03, -1.64779844e-02,\n",
       "        4.26931993e-03, -1.10123190e-03, -8.59967554e-03, -5.73022018e-04,\n",
       "       -1.22711887e-02,  5.53994631e-04,  2.49836019e-03, -5.82307512e-05,\n",
       "        8.52810174e-04, -5.39991523e-04,  1.46007435e-02, -9.40587055e-03,\n",
       "        2.27169242e-03,  1.43457990e-02,  5.77062453e-03, -2.63337458e-03,\n",
       "       -8.44673015e-04, -6.11469827e-04,  1.19820955e-02, -1.44675225e-03,\n",
       "        3.50631175e-03, -1.00676633e-02,  1.67701350e-04,  8.27965734e-03,\n",
       "       -1.70282612e-03,  3.63461163e-03,  6.09581642e-03,  5.84355835e-04,\n",
       "        2.33237254e-03, -5.61803416e-04, -9.30497745e-03,  4.63728025e-03,\n",
       "       -1.15865733e-03,  1.02945558e-03, -1.93415039e-04, -2.87219467e-03,\n",
       "       -7.09495427e-04,  1.74084277e-03, -5.29342944e-03,  8.29191239e-03,\n",
       "       -1.43735668e-03,  3.01508049e-03,  1.74134550e-03, -4.71510689e-04,\n",
       "        2.79765767e-02, -1.47443739e-02, -3.81518063e-03, -5.48015570e-03,\n",
       "       -3.63401083e-03,  2.78448015e-03,  8.04695852e-03, -6.47806621e-06,\n",
       "        7.72097234e-04, -1.70181479e-03,  5.78755095e-03,  3.57748430e-03,\n",
       "       -1.31834161e-03, -5.29167044e-03, -3.68290312e-03, -1.36365091e-02,\n",
       "       -3.94333966e-03, -4.80141945e-03, -3.90552765e-03, -7.00239350e-03,\n",
       "        1.59931120e-02, -2.95926736e-03, -5.63330077e-03, -3.95643938e-04,\n",
       "        1.63791516e-03, -3.60709022e-03,  1.10283308e-02,  1.92648133e-03,\n",
       "        1.85435726e-03, -7.15771363e-03, -3.50632844e-03,  1.79015719e-03,\n",
       "        5.04994094e-03,  1.21846429e-02, -4.92267133e-03, -1.52209390e-03,\n",
       "        5.88554121e-03, -2.42504964e-03, -4.21539915e-03, -8.68003236e-04,\n",
       "        1.15552134e-02, -6.60128936e-04, -2.14447867e-03, -5.56916026e-04,\n",
       "        1.02214395e-02,  3.03905510e-02,  5.95601650e-03,  2.02517234e-03,\n",
       "       -3.01359304e-03, -1.31818287e-02, -2.54873432e-04, -7.32159302e-03,\n",
       "       -7.56506256e-03,  1.41140382e-03,  5.73425175e-03, -1.84005858e-02,\n",
       "       -1.59047346e-03,  2.51108082e-03,  2.81149758e-03,  1.04691617e-02,\n",
       "       -7.56963979e-04,  3.24414718e-03, -1.30711725e-02,  3.25570380e-03,\n",
       "        1.32269505e-02, -4.24403850e-03, -3.51085875e-03, -4.97454457e-03,\n",
       "       -2.17424798e-03, -3.56663594e-03, -1.22100434e-03, -4.30380538e-03,\n",
       "       -2.34895646e-04, -8.51968350e-03, -8.91044289e-04,  1.32534255e-02,\n",
       "       -2.71629364e-03,  8.65770271e-03,  9.70042865e-04, -3.42199707e-03,\n",
       "       -1.68990821e-03, -1.39281298e-02, -9.63036005e-04,  1.29926042e-02,\n",
       "       -6.54145728e-03, -1.62193221e-02,  1.07536381e-02, -1.41911813e-03,\n",
       "       -1.87795511e-03, -5.10144870e-03, -1.38670940e-02,  2.61003921e-03,\n",
       "       -1.31304199e-03,  7.44631192e-04, -1.01071307e-03, -2.57416170e-03,\n",
       "        3.10532304e-03, -5.16557231e-03,  9.70925518e-04,  1.64163886e-02,\n",
       "        7.29721221e-03, -4.55111602e-03,  1.52999192e-02,  4.24889109e-04,\n",
       "        4.63869372e-03, -2.35146943e-03, -8.16077234e-03, -6.30402818e-03,\n",
       "        2.18716753e-02,  1.00948304e-02, -4.20216797e-04, -4.58356245e-04,\n",
       "        1.72605069e-03, -5.92640112e-03, -8.63652642e-04, -1.10394891e-02,\n",
       "        7.59789780e-04,  5.35159313e-04, -5.04214620e-05, -5.24081531e-03,\n",
       "        4.60119520e-03, -2.27072486e-04, -6.68117778e-04, -1.08001226e-03,\n",
       "        8.94493647e-04, -1.85777145e-03, -1.25585975e-03,  3.00177854e-03,\n",
       "       -3.62143344e-03, -3.17293543e-03,  9.80983308e-03,  1.99334820e-02,\n",
       "        1.65676130e-02,  2.77770648e-02,  1.62422986e-02, -7.37376384e-03,\n",
       "        1.94361520e-03,  3.76859915e-03, -4.06819451e-04,  1.50713015e-02,\n",
       "       -7.69055322e-03, -2.48104575e-04,  3.55325917e-03,  3.65651305e-03,\n",
       "        1.48066385e-03, -5.76241777e-03, -1.27855803e-02,  1.17694570e-02,\n",
       "        1.81296642e-03, -1.38657581e-02,  1.35062979e-03,  1.47988470e-02,\n",
       "        5.06830511e-03, -3.23914185e-03, -3.07089099e-03, -1.01161741e-02,\n",
       "       -4.86165582e-03,  1.26076959e-03, -1.46001903e-03,  7.42711400e-03,\n",
       "       -7.03102228e-04, -4.86798601e-03,  1.47992148e-03, -7.98093296e-03,\n",
       "        2.62138861e-03,  1.64151863e-03,  1.62749445e-02,  9.00316738e-03,\n",
       "       -5.12693187e-03,  2.83341167e-03, -4.16086782e-03,  7.82833568e-04,\n",
       "        3.72729249e-04,  4.06168653e-03,  7.19687572e-04,  5.82881739e-03,\n",
       "        5.73823513e-03,  1.15073792e-02,  3.37178409e-03, -8.96851472e-03,\n",
       "       -8.09939718e-04, -9.83052254e-03, -2.29605521e-03,  2.37212903e-03,\n",
       "       -3.32112841e-03, -2.91265453e-04,  6.63091039e-03, -6.91457920e-03,\n",
       "        9.98772964e-03,  3.13021946e-03,  6.87601635e-03, -1.25724464e-03,\n",
       "        1.22476400e-03, -3.91335339e-03, -1.20819033e-02,  1.52257565e-03,\n",
       "       -4.25900118e-04, -6.57063629e-03,  1.62813022e-04,  4.32776616e-03,\n",
       "       -3.93133728e-03,  7.65378326e-04, -5.48246953e-04,  1.04351806e-02,\n",
       "        7.90718483e-04, -2.93889724e-03, -6.12545805e-03,  2.21661484e-02,\n",
       "       -1.17176541e-03, -3.05639041e-03,  1.23656008e-03,  3.43657383e-03,\n",
       "        2.20558504e-03, -1.38034949e-03, -1.89403119e-03, -4.30636370e-04,\n",
       "        1.06526001e-03, -2.20983678e-03, -3.15488722e-04,  3.49580618e-03,\n",
       "        2.48686141e-03, -1.92133677e-03,  3.83208274e-03, -1.70099745e-03,\n",
       "       -9.56735826e-04,  3.25857761e-03, -5.69106795e-03,  4.28163045e-03,\n",
       "        1.86057760e-03,  1.51993537e-04,  3.26426296e-03,  7.53647904e-03,\n",
       "        2.11967807e-02, -4.55691828e-03, -4.33479403e-03,  7.66146286e-03,\n",
       "       -1.70447743e-02, -9.29847210e-04,  4.70168425e-03, -9.58094538e-03,\n",
       "        9.60046084e-04, -1.06055027e-02,  1.00145257e-02, -3.32710488e-03,\n",
       "        5.83889703e-03, -3.71964770e-03, -3.82212728e-03,  7.90255607e-03,\n",
       "       -1.01415308e-02, -6.24843922e-03, -1.18773006e-03, -1.82071253e-03,\n",
       "       -7.72142113e-03, -3.97616385e-03,  6.24359507e-03, -1.98263184e-03])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_metrics_dec_baseline['dimension_correlation'] - correlation_metrics_decoding['dimension_correlation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d58a45-678c-4a1c-a0ca-022634494cb4",
   "metadata": {},
   "source": [
    "Seeing basically no difference here isn't super surprising since decoding is a more difficult task.\n",
    "\n",
    "That though highlights where we have to keep improving. Have fun working on the project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
